# ===== __init__.py =====
# text_classifier/__init__.py
"""
Text Classification System

A flexible system for classifying open-text survey responses using LLMs.
"""

# Import main API functions
from .api import (
    classify_texts,
    validate_classification,
    load_classification_results,
    load_validation_results,
    compare_runs,
    list_all_runs,
    get_run_info,
    load_config,
    generate_categories_only,  
    load_saved_categories,
    classify_texts_hybrid,
    load_setfit_model,
    translate_dataset      
)

# Import classes for advanced usage
from .classifier import TextClassifier
from .validator import ClassificationValidator
from .storage import RunStorage
from .models import ClassificationRun, ValidationRun

# Import SetFit classes
from .setfit_classifier import SetFitClassifier, HybridClassifier, SETFIT_AVAILABLE

__version__ = "0.1.0"

__all__ = [
    # Main API
    "classify_texts",
    "validate_classification",
    "load_classification_results",
    "load_validation_results",
    "compare_runs",
    "list_all_runs",
    "get_run_info",
    "load_config",
    "generate_categories_only",  
    "load_saved_categories",
    "classify_texts_hybrid",
    "load_setfit_model",
    "translate_dataset"
    
    # Classes
    "TextClassifier",
    "ClassificationValidator",
    "RunStorage",
    "ClassificationRun",
    "ValidationRun",
    
    # SetFit classes
    "SetFitClassifier",
    "HybridClassifier",
    "SETFIT_AVAILABLE"
]

# ===== api.py =====
# text_classifier/api.py
"""
High-level API for text classification system.
This module provides the main functions users should interact with.
"""

from pathlib import Path
from datetime import datetime
import uuid
import json
from typing import Dict, Any, Optional, List, Union, Tuple  # Added Union and Tuple
import pandas as pd

from .models import ClassificationRun, ValidationRun
from .storage import RunStorage
from .classifier import TextClassifier
from .validator import ClassificationValidator
from .setfit_classifier import SetFitClassifier, HybridClassifier, SETFIT_AVAILABLE
from .config import (
    load_config as _load_config,
    validate_config,
    get_config_with_defaults,
    parse_categories
)

def classify_texts(
    config: Dict[str, Any],
    run_id: Optional[str] = None,
    storage_dir: Path = Path("./runs")
) -> str:
    """
    Run text classification with proper storage.
    """
    config = get_config_with_defaults(config)
    validate_config(config)
    
    if run_id is None:
        run_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    
    storage = RunStorage(storage_dir)
    
    df = pd.read_csv(config["file_path"],keep_default_na=False)

    original_count = len(df)
    df = df[df[config["text_column"]].str.len() >= 1]
    df = df.reset_index(drop=True) # Reset index here as well for consistency
    dropped_count = original_count - len(df)
    
    if dropped_count > 0:
        print(f"[*] Dropped {dropped_count} rows with empty text")

    categories = parse_categories(config)
    
    classifier = TextClassifier(
        config["classifier_model"],
        config["classifier_backend"]
    )
    
    classified_df, final_categories, metrics = classifier.run_classification(
        df=df,
        text_column=config["text_column"],
        id_column=config["id_column"],
        categories=categories,
        multiclass=config["multiclass"],
        n_samples=config["n_samples"],
        question_context=config["question_context"],
        category_model=config["category_model"],
        category_backend=config["category_backend"]
    )

    metrics["dropped_empty_rows"] = dropped_count
    metrics["original_rows"] = original_count    
    
    run = ClassificationRun(
        run_id=run_id,
        timestamp=datetime.now(),
        config=config,
        input_file=Path(config["file_path"]),
        categories=final_categories,
        metrics=metrics
    )
    
    output_file = storage.save_classification_run(run, classified_df)
    print(f"[*] Classification complete. Run ID: {run_id}")
    print(f"[*] Results saved to: {output_file}")
    
    return run_id

def validate_classification(
    classification_run_id: str,
    config: Optional[Dict[str, Any]] = None,
    sample_size: Optional[int] = None,
    storage_dir: Path = Path("./runs")
) -> str:
    """
    Validate a classification run using LLM-as-judge.
    Supports both single category and multiclass validation.
    
    Args:
        classification_run_id: ID of the classification run to validate
        config: Optional config to override validation settings
        sample_size: Number of samples to validate (None = all)
        storage_dir: Directory where runs are stored
        
    Returns:
        validation_id: Unique identifier for this validation run
    """
    # Initialize storage
    storage = RunStorage(storage_dir)
    
    # Load classification run
    class_run = storage.get_classification_run(classification_run_id)
    if not class_run:
        raise ValueError(f"Classification run {classification_run_id} not found")
    
    # Load classification data
    classified_df = storage.load_classification_data(classification_run_id)
    if classified_df is None:
        raise ValueError(f"Could not load data for run {classification_run_id}")
    
    # Use config from classification run if not provided
    if config is None:
        config = class_run.config.copy()
    
    # Override sample size if provided
    if sample_size is not None:
        config["validation_samples"] = sample_size
    
    # Initialize validator
    validator = ClassificationValidator(
        config["judge_model"],
        config["judge_backend"]  # Changed from config.get("backend", "ollama")
    )
    
    # Determine if multiclass
    multiclass = class_run.config.get("multiclass", False)
    
    # Run validation
    if multiclass:
        # For multiclass, we don't have a single category column
        validated_df, metrics = validator.validate_classification_run(
            df=classified_df,
            text_column=class_run.config["text_column"],
            category_column=None,
            categories=class_run.categories,
            sample_size=config.get("validation_samples"),
            multiclass=True
        )
    else:
        # Single category validation
        validated_df, metrics = validator.validate_classification_run(
            df=classified_df,
            text_column=class_run.config["text_column"],
            category_column="category",
            categories=class_run.categories,
            sample_size=config.get("validation_samples"),
            multiclass=False
        )
    
    # Generate validation ID
    val_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    
    # Save validation results
    val_dir = storage.base_dir / f"validation_{val_id}"
    val_dir.mkdir(exist_ok=True)
    
    output_file = val_dir / f"validation_{val_id}.csv"
    validated_df.to_csv(output_file, index=False)
    
    # Save validation metadata
    val_run = {
        "validation_id": val_id,
        "classification_run_id": classification_run_id,
        "timestamp": datetime.now().isoformat(),
        "config": config,
        "results_file": str(output_file),
        "metrics": metrics,
        "multiclass": multiclass
    }
    
    storage.metadata["validation_runs"][val_id] = val_run
    storage._save_metadata()
    
    print(f"[*] Validation complete. ID: {val_id}")
    print(f"[*] Average quality score: {metrics['average_score']:.2f}")
    
    if multiclass and "category_scores" in metrics:
        print(f"[*] Per-category scores:")
        for cat, score in metrics["category_scores"].items():
            print(f"    - {cat}: {score:.2f}")
    
    print(f"[*] Results saved to: {output_file}")
    
    return val_id

def load_classification_results(
    run_id: str,
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Load classification results for a given run.
    
    Args:
        run_id: Classification run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with classification results
    """
    storage = RunStorage(storage_dir)
    df = storage.load_classification_data(run_id)
    if df is None:
        raise ValueError(f"Could not load results for run {run_id}")
    return df


def load_validation_results(
    validation_id: str,
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Load validation results.
    
    Args:
        validation_id: Validation run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with validation results
    """
    storage = RunStorage(storage_dir)
    val_run = storage.metadata.get("validation_runs", {}).get(validation_id)
    if not val_run:
        raise ValueError(f"Validation run {validation_id} not found")
    
    results_file = Path(val_run["results_file"])
    if not results_file.exists():
        raise ValueError(f"Results file not found: {results_file}")
    
    return pd.read_csv(results_file)


def compare_runs(
    run_ids: List[str],
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Compare multiple classification runs.
    
    Args:
        run_ids: List of classification run IDs to compare
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with comparison metrics
    """
    storage = RunStorage(storage_dir)
    
    comparisons = []
    for run_id in run_ids:
        run = storage.get_classification_run(run_id)
        if run:
            comparison = {
                "run_id": run_id,
                "timestamp": run.timestamp,
                "model": run.config.get("classifier_model"),
                "backend": run.config.get("backend"),
                "multiclass": run.config.get("multiclass", False),
                "num_categories": len(run.categories),
                "total_rows": run.metrics.get("total_rows", 0),
                "classified_rows": run.metrics.get("classified_rows", 0)
            }
            
            # Add validation metrics if available
            validations = [v for v in storage.metadata.get("validation_runs", {}).values()
                          if v["classification_run_id"] == run_id]
            if validations:
                latest_val = max(validations, key=lambda x: x["timestamp"])
                comparison["validation_score"] = latest_val["metrics"]["average_score"]
                comparison["validated_samples"] = latest_val["metrics"]["total_validated"]
            
            comparisons.append(comparison)
    
    # Create comparison DataFrame
    comparison_df = pd.DataFrame(comparisons)
    
    print("\n=== Run Comparison ===")
    print(comparison_df.to_string())
    
    return comparison_df


def list_all_runs(
    run_type: str = "classification",
    storage_dir: Path = Path("./runs")
) -> List[Dict[str, Any]]:
    """
    List all runs of a given type.
    
    Args:
        run_type: "classification" or "validation"
        storage_dir: Directory where runs are stored
        
    Returns:
        List of run metadata dictionaries
    """
    storage = RunStorage(storage_dir)
    runs = storage.list_runs(run_type)
    
    # Sort by timestamp (newest first)
    runs.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
    
    return runs


def get_run_info(
    run_id: str,
    storage_dir: Path = Path("./runs")
) -> Dict[str, Any]:
    """
    Get detailed information about a specific run.
    
    Args:
        run_id: Classification or validation run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        Dictionary with run information
    """
    storage = RunStorage(storage_dir)
    
    # Check classification runs
    run = storage.get_classification_run(run_id)
    if run:
        return {
            "type": "classification",
            "run": run.to_dict(),
            "validations": [v for v in storage.metadata.get("validation_runs", {}).values()
                           if v["classification_run_id"] == run_id]
        }
    
    # Check validation runs
    val_run = storage.metadata.get("validation_runs", {}).get(run_id)
    if val_run:
        return {
            "type": "validation",
            "run": val_run,
            "classification_run": storage.get_classification_run(
                val_run["classification_run_id"]
            ).to_dict() if val_run.get("classification_run_id") else None
        }
    
    raise ValueError(f"Run {run_id} not found")

def generate_categories_only(
    config: Dict[str, Any],
    save_to_file: bool = True,
    storage_dir: Path = Path("./runs")
) -> List[str]:
    """
    Generate categories from data without running classification.
    
    Args:
        config: Configuration dictionary with keys:
            - file_path: Path to input CSV
            - text_column: Column containing text
            - n_samples: Number of samples for category generation (default: 100)
            - question_context: Context for category generation
            - category_model: Model for generating categories (default: classifier_model)
            - category_backend: Backend for category generation (default: classifier_backend)
        save_to_file: Whether to save categories to a JSON file
        storage_dir: Directory to save categories file
        
    Returns:
        List of generated categories
        
    Example:
        >>> config = {
        ...     "file_path": "survey.csv",
        ...     "text_column": "response",
        ...     "question_context": "What features would you like to see?"
        ... }
        >>> categories = generate_categories_only(config)
        >>> print(categories)
        ['Feature Request', 'Bug Report', 'Positive Feedback', ...]
    """
    # Apply defaults but only validate required fields for category generation
    config_with_defaults = get_config_with_defaults(config)
    
    # Minimal validation - we don't need id_column for category generation
    required = ["file_path", "text_column"]
    missing = [key for key in required if key not in config]
    if missing:
        raise ValueError(f"Missing required config parameters: {missing}")
    
    # Validate file exists
    file_path = Path(config_with_defaults["file_path"])  # Use config_with_defaults
    if not file_path.exists():
        raise FileNotFoundError(f"Input file not found: {file_path}")
    
    # Load data
    print(f"[*] Loading data from {file_path}")
    df = pd.read_csv(file_path, keep_default_na=False)
    
    # Drop empty rows
    original_count = len(df)
    df = df[df[config_with_defaults["text_column"]].str.len() >= 1]  # Use config_with_defaults
    dropped_count = original_count - len(df)
    
    if dropped_count > 0:
        print(f"[*] Dropped {dropped_count} rows with empty text")
    
    print(f"[*] Total rows available: {len(df)}")
    
    # Initialize classifier for category generation
    classifier = TextClassifier(
        config_with_defaults.get("category_model") or config_with_defaults["classifier_model"],
        config_with_defaults.get("category_backend") or config_with_defaults["classifier_backend"]
    )
    
    # Generate categories
    categories = classifier.generate_categories(
        df=df,
        text_column=config_with_defaults["text_column"],  # Use config_with_defaults
        n_samples=config_with_defaults.get("n_samples", 100),
        question_context=config_with_defaults.get("question_context", "")
    )
    
    print(f"\n[*] Generated {len(categories)} categories:")
    for i, cat in enumerate(categories, 1):
        print(f"    {i}. {cat}")
    
    # Save categories if requested
    if save_to_file:
        storage_dir = Path(storage_dir)
        storage_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        cat_file = storage_dir / f"categories_{timestamp}.json"
        
        save_data = {
            "categories": categories,
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "source_file": str(file_path),
                "text_column": config_with_defaults["text_column"],  # Use config_with_defaults
                "n_samples": config_with_defaults.get("n_samples", 100),
                "question_context": config_with_defaults.get("question_context", ""),
                "model": config_with_defaults.get("category_model") or config_with_defaults["classifier_model"],
                "backend": config_with_defaults.get("category_backend") or config_with_defaults["classifier_backend"],
                "total_rows": len(df),
                "dropped_empty_rows": dropped_count
            }
        }
        
        with open(cat_file, 'w') as f:
            json.dump(save_data, f, indent=2)
        
        print(f"\n[*] Categories saved to: {cat_file}")
    
    return categories

def load_saved_categories(
    categories_file: Union[str, Path]
) -> Tuple[List[str], Dict[str, Any]]:
    """
    Load categories from a previously saved categories file.
    
    Args:
        categories_file: Path to the categories JSON file
        
    Returns:
        Tuple of (categories list, metadata dict)
        
    Example:
        >>> categories, metadata = load_saved_categories("./runs/categories_20231230_143022.json")
        >>> print(f"Loaded {len(categories)} categories generated on {metadata['timestamp']}")
    """
    categories_file = Path(categories_file)
    
    if not categories_file.exists():
        raise FileNotFoundError(f"Categories file not found: {categories_file}")
    
    with open(categories_file) as f:
        data = json.load(f)
    
    return data["categories"], data.get("metadata", {})

# Convenience function for loading config from file
def load_config(config_path: str = "config.json") -> Dict[str, Any]:
    """Load configuration from JSON file."""
    return _load_config(config_path)

# Add to imports at the top of api.py
from .setfit_classifier import SetFitClassifier, HybridClassifier, SETFIT_AVAILABLE

# Add new function after the existing ones
def classify_texts_hybrid(
    config: Dict[str, Any],
    run_id: Optional[str] = None,
    storage_dir: Path = Path("./runs"),
    train_config: Optional[Dict[str, Any]] = None
) -> str:
    """
    Run text classification using hybrid LLM + SetFit approach.
    Supports both single-label and multi-label classification.
    """
    if not SETFIT_AVAILABLE:
        raise ImportError("SetFit not available. Run: pip install setfit sentence-transformers")
    
    config = get_config_with_defaults(config)
    validate_config(config)
    
    is_multiclass = config.get("multiclass", False)
    
    setfit_defaults = {
        "use_setfit": True,
        "setfit_model": "sentence-transformers/paraphrase-mpnet-base-v2",
        "confidence_threshold": 0.5,
        "max_llm_samples": 200,
        "min_samples_per_category": 10
    }
    
    for key, value in setfit_defaults.items():
        if key not in config:
            config[key] = value
    
    if run_id is None:
        run_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    
    storage = RunStorage(storage_dir)
    run_dir = storage.base_dir / f"classification_{run_id}"
    run_dir.mkdir(exist_ok=True)
    
    df = pd.read_csv(config["file_path"], keep_default_na=False)
    original_count = len(df)
    df = df[df[config["text_column"]].str.len() >= 1]
    
    # *** THE FIX IS HERE ***
    # Reset the index after filtering to ensure it's a clean, sequential 0, 1, 2...
    df = df.reset_index(drop=True)
    
    dropped_count = original_count - len(df)
    
    if dropped_count > 0:
        print(f"[*] Dropped {dropped_count} rows with empty text")
    
    categories = parse_categories(config)
    
    llm_classifier = TextClassifier(
        config["classifier_model"],
        config["classifier_backend"]
    )
    
    if categories is None:
        cat_classifier = TextClassifier(
            model_name=config.get("category_model") or config["classifier_model"],
            backend=config.get("category_backend") or config["classifier_backend"]
        )
        categories = cat_classifier.generate_categories(
            df, config["text_column"], config["n_samples"], config["question_context"]
        )
        print(f"[*] Generated categories: {', '.join(categories)}")
    
    hybrid = HybridClassifier(
        llm_classifier,
        config["setfit_model"],
        config["confidence_threshold"],
        config["min_samples_per_category"],
        config["max_llm_samples"]
    )
    
    print("\n=== Phase 1: Collecting training data with LLM ===")
    training_df, training_metrics = hybrid.collect_training_data(
        df.sample(frac=1, random_state=42),
        config["text_column"],
        config["id_column"],
        categories,
        config["question_context"],
        multiclass=is_multiclass
    )
    
    training_file = run_dir / "training_data.csv"
    training_df.to_csv(training_file, index=False)
    
    print("\n=== Phase 2: Training SetFit model ===")
    setfit_metrics = hybrid.train_setfit(multiclass=is_multiclass)
    
    setfit_dir = run_dir / "setfit_model"
    hybrid.setfit.save(setfit_dir)
    
    print("\n=== Phase 3: Hybrid classification ===")
    classified_df, classification_metrics = hybrid.classify_hybrid(
        df,
        config["text_column"],
        config["id_column"],
        config["question_context"],
        multiclass=is_multiclass
    )
    
    metrics = {
        "total_rows": len(df),
        "dropped_empty_rows": dropped_count,
        "original_rows": original_count,
        "training_metrics": training_metrics,
        "setfit_metrics": setfit_metrics,
        "classification_metrics": classification_metrics,
        "hybrid_mode": True,
        "num_categories": len(categories)
    }
    
    run = ClassificationRun(
        run_id=run_id,
        timestamp=datetime.now(),
        config=config,
        input_file=Path(config["file_path"]),
        categories=categories,
        metrics=metrics
    )
    
    output_file = storage.save_classification_run(run, classified_df)
    
    print(f"\n[*] Hybrid classification complete. Run ID: {run_id}")
    print(f"[*] Results saved to: {output_file}")
    print(f"[*] LLM used for: {classification_metrics['llm_percentage']:.1f}% of classifications")
    
    return run_id

def load_setfit_model(
    run_id: str,
    storage_dir: Path = Path("./runs")
) -> SetFitClassifier:
    """
    Load a trained SetFit model from a previous run.
    
    Args:
        run_id: Classification run ID that used SetFit
        storage_dir: Directory where runs are stored
        
    Returns:
        Loaded SetFitClassifier instance
    """
    run_dir = storage_dir / f"classification_{run_id}" / "setfit_model"
    
    if not run_dir.exists():
        raise ValueError(f"No SetFit model found for run {run_id}")
    
    classifier = SetFitClassifier()
    classifier.load(run_dir)
    
    return classifier

def translate_dataset(
    file_path: str,
    text_column: str,
    lang_column: str,
    lang_to_translate: str = "FR-CA",
    target_lang_code: str = "en",
    model_name: str = "Helsinki-NLP/opus-mt-fr-en",
    batch_size: int = 32
) -> str:
    """
    Translates a specific language in a dataset to a target language.

    Args:
        file_path (str): Path to the input CSV file.
        text_column (str): The column containing text to translate.
        lang_column (str): The column that specifies the language of the text.
        lang_to_translate (str): The language code to filter and translate (e.g., "FR-CA").
        target_lang_code (str): The target language code (e.g., "en").
        model_name (str): The Hugging Face model to use for translation.
        batch_size (int): The number of texts to translate at once.

    Returns:
        str: The path to the new, translated CSV file.
    """
    if not TRANSLATION_AVAILABLE:
        raise ImportError(
            "Translation dependencies not installed. Run: pip install transformers torch sentencepiece"
        )

    print(f"[*] Loading dataset from: {file_path}")
    p = Path(file_path)
    df = pd.read_csv(p)

    # Filter the DataFrame to get only the rows that need translation
    to_translate_df = df[df[lang_column] == lang_to_translate]
    
    if to_translate_df.empty:
        print(f"[*] No rows found with language '{lang_to_translate}'. No translation needed.")
        return file_path

    print(f"[*] Found {len(to_translate_df)} rows to translate from '{lang_to_translate}' to '{target_lang_code}'.")
    
    # Load the translation pipeline
    print(f"[*] Loading translation model: {model_name}")
    device = 0 if torch.cuda.is_available() else -1 # Use GPU if available
    translator = pipeline(
        "translation",
        model=model_name,
        device=device
    )

    # Extract the texts to translate
    texts_to_translate = to_translate_df[text_column].tolist()

    # Translate in batches for efficiency and progress tracking
    print("[*] Translating texts...")
    translated_texts = []
    for i in tqdm(range(0, len(texts_to_translate), batch_size), desc="Translating"):
        batch = texts_to_translate[i:i+batch_size]
        translated_batch = translator(batch, src_lang=lang_to_translate, tgt_lang=target_lang_code)
        translated_texts.extend([item['translation_text'] for item in translated_batch])

    # Overwrite the original text column with the translated text
    # Use .loc to ensure we are modifying the original DataFrame correctly
    df.loc[to_translate_df.index, text_column] = translated_texts
    
    # Create the new filename
    output_filename = f"{p.stem}_translated.csv"
    output_path = p.parent / output_filename
    
    # Save the modified DataFrame
    print(f"[*] Saving translated dataset to: {output_path}")
    df.to_csv(output_path, index=False)
    
    return str(output_path)

# ===== classifier.py =====
# text_classifier/classifier.py
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
from tqdm import tqdm
import os
import time
import re

try:
    import ollama
except ImportError:
    ollama = None
    
try:
    import openai
except ImportError:
    openai = None


class TextClassifier:
    def __init__(self, model_name: str, backend: str = "ollama", max_retries: int = 3):
        self.model_name = model_name
        self.backend = backend
        self.max_retries = max_retries

        if backend == "openai":
            if openai is None:
                raise ImportError("OpenAI package not installed. Run: pip install openai")
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("OPENAI_API_KEY environment variable not set")
            self.client = openai.OpenAI()
        elif backend == "ollama":
            if ollama is None:
                raise ImportError("Ollama package not installed. Run: pip install ollama")
    
    def send_chat(self, messages: List[Dict[str, Any]], retry_count: int = 0) -> Dict[str, Any]:
        """Send chat request to LLM with retry logic"""
        try:
            if self.backend == "openai":
                try:
                    resp = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=messages,
                    )
                    content = resp.choices[0].message.content
                    return {"message": {"content": content}}
                except openai.RateLimitError as e:
                    if retry_count < self.max_retries:
                        wait_time = 2 ** retry_count  # Exponential backoff
                        print(f"Rate limit hit, waiting {wait_time}s...")
                        time.sleep(wait_time)
                        return self.send_chat(messages, retry_count + 1)
                    raise
                except openai.APIError as e:
                    if retry_count < self.max_retries:
                        print(f"API error, retrying... ({retry_count + 1}/{self.max_retries})")
                        time.sleep(1)
                        return self.send_chat(messages, retry_count + 1)
                    raise
            else:  # ollama
                try:
                    return ollama.chat(model=self.model_name, messages=messages)
                except Exception as e:
                    if retry_count < self.max_retries:
                        print(f"Ollama error, retrying... ({retry_count + 1}/{self.max_retries})")
                        time.sleep(1)
                        return self.send_chat(messages, retry_count + 1)
                    raise
                    
        except Exception as e:
            print(f"Failed after {self.max_retries} retries: {e}")
            raise
    
    def generate_categories(
        self,
        df: pd.DataFrame,
        text_column: str,
        n_samples: int = 100,
        question_context: str = ""
    ) -> List[str]:
        """Generate categories from sample responses with robust parsing"""
        print(f"[*] Sampling {n_samples} responses to generate categories...")
        sample = df[text_column].dropna().sample(n=min(n_samples, len(df)))
        sample_text = "\n\n".join(sample)
        
        # More explicit prompt format
        prompt = f"""The following are sample survey responses to the survey question: {question_context}

{sample_text}

Based on these responses, generate 5-15 mutually exclusive categories that summarise the main themes.

IMPORTANT: Format your response as a numbered list, one category per line:
1. First Category
2. Second Category
3. Third Category
...

Include a "Don't know/Uncertain" or "Other" category if appropriate.
Each category should be concise (2-5 words) and clearly distinct from others."""
        
        try:
            reply = self.send_chat([{"role": "system", "content": prompt}])
            text = reply["message"]["content"].strip()
            
            # Parse categories with multiple strategies
            categories = self._parse_categories(text)
            
            if not categories:
                print("[!] Warning: Could not parse categories from LLM response")
                # Fallback to generic categories
                categories = ["Positive", "Negative", "Neutral", "Other"]
            
            # Validate and clean categories
            categories = self._validate_categories(categories)
            
            return categories
            
        except Exception as e:
            print(f"[!] Error generating categories: {e}")
            # Return sensible defaults
            return ["Positive", "Negative", "Neutral", "Other"]

    def _validate_categories(self, categories: List[str]) -> List[str]:
        """Validate and clean the parsed categories"""
        cleaned = []
        seen = set()
        
        for cat in categories:
            # Remove any remaining numbering or bullets
            cat = re.sub(r'^\d+[\.\)]\s*', '', cat)
            cat = re.sub(r'^[-*•]\s*', '', cat)
            
            # Remove quotes
            cat = cat.strip('"\'')
            
            # Remove trailing punctuation
            cat = cat.rstrip('.,;:')
            
            # Normalize whitespace
            cat = ' '.join(cat.split())
            
            # Skip if too short or too long
            if len(cat) < 2 or len(cat) > 100:
                continue
            
            # Skip duplicates (case-insensitive)
            cat_lower = cat.lower()
            if cat_lower in seen:
                continue
            seen.add(cat_lower)
            
            cleaned.append(cat)
        
        # Ensure we have at least some categories
        if len(cleaned) < 3:
            print(f"[!] Warning: Only {len(cleaned)} valid categories found")
            # Add some generic ones if needed
            generic = ["Other", "Uncertain", "Mixed"]
            for g in generic:
                if g.lower() not in seen:
                    cleaned.append(g)
                    if len(cleaned) >= 5:
                        break
        
        # Cap at reasonable number
        if len(cleaned) > 15:
            print(f"[!] Warning: {len(cleaned)} categories found, capping at 15")
            cleaned = cleaned[:15]
        
        return cleaned

    def _parse_categories(self, text: str) -> List[str]:
        """Parse categories from LLM response using multiple strategies"""
        categories = []
        
        # Strategy 1: Parse numbered list (1. Category, 2. Category, etc.)
        numbered_pattern = r'^\s*\d+[\.\)]\s*(.+)$'
        lines = text.strip().split('\n')
        
        for line in lines:
            match = re.match(numbered_pattern, line.strip())
            if match:
                categories.append(match.group(1).strip())
        
        if categories:
            return categories
        
        # Strategy 2: Parse bullet points (- Category, * Category, • Category)
        bullet_pattern = r'^\s*[-*•]\s*(.+)$'
        for line in lines:
            match = re.match(bullet_pattern, line.strip())
            if match:
                categories.append(match.group(1).strip())
        
        if categories:
            return categories
        
        # Strategy 3: Parse comma-separated list
        # Look for a line that contains multiple comma-separated items
        for line in lines:
            if ',' in line:
                # Clean up common prefixes
                line = re.sub(r'^(Categories:|The categories are:|Here are the categories:)', '', line, flags=re.IGNORECASE)
                line = line.strip()
                
                # Split by comma and clean each item
                potential_categories = [
                    item.strip().strip('"\'') 
                    for item in line.split(',') 
                    if item.strip()
                ]
                
                # Validate these look like categories (not too long)
                if all(len(cat) < 50 for cat in potential_categories) and len(potential_categories) >= 3:
                    categories = potential_categories
                    break
        
        if categories:
            return categories
        
        # Strategy 4: Each non-empty line as a category (if reasonable number)
        # Filter out common non-category lines
        exclude_patterns = [
            r'^(here|these|the|based on|categories|following)',
            r'^(include|should|must|note)',
            r'^\d+$',  # Just numbers
            r'^[a-z]$',  # Single lowercase letters
        ]
        
        potential_categories = []
        for line in lines:
            line = line.strip()
            if line and len(line) < 50:  # Reasonable length for category
                # Check if line should be excluded
                if not any(re.match(pattern, line.lower()) for pattern in exclude_patterns):
                    potential_categories.append(line)
        
        # If we got a reasonable number of categories this way
        if 3 <= len(potential_categories) <= 20:
            return potential_categories
        
        return categories

    def _disambiguate_category(self, text: str, ambiguous_categories: List[str], question_context: str) -> str:
        """
        Makes a second, targeted LLM call to resolve ambiguity between multiple matching categories.
        """
        print(f"[*] Ambiguity detected. Asking LLM to choose from: {ambiguous_categories}")
        
        prompt = (
            "You are a classification assistant. Your task is to resolve an ambiguity.\n"
            f"Survey question: {question_context}\n\n"
            f"Response:\n\"{text}\"\n\n"
            "The response could fit into several categories. Choose the single best category from the following list ONLY:\n"
            f"- {', '.join(ambiguous_categories)}\n\n"
            "Return the single best category name exactly as it appears in the list."
        )

        try:
            reply = self.send_chat([{"role": "system", "content": prompt}])
            response = reply["message"]["content"].strip()
            
            # Final check: The response from the disambiguation call MUST be a perfect match
            # to one of the ambiguous categories.
            for cat in ambiguous_categories:
                if cat.lower() == response.lower():
                    return cat # Return the original category name with correct casing
            
            print(f"[!] Warning: Disambiguation failed. LLM returned '{response}', which is not in the provided list.")
            return "Uncategorized"

        except Exception as e:
            print(f"Error during disambiguation call: {e}")
            return "Uncategorized"

    def classify_single(self, text: str, categories: List[str], question_context: str = "") -> str:
        """
        Classify a single text with a robust, multi-step validation and disambiguation process.
        """
        prompt = (
            "You are a survey response classifier.\n"
            f"Survey question: {question_context}\n\n"
            f"Response:\n\"{text}\"\n\n"
            f"Choose the best category among:\n{', '.join(categories)}\n\n"
            "Return the category name only."
        )
        
        try:
            reply = self.send_chat([{"role": "system", "content": prompt}])
            response_text = reply["message"]["content"].strip()
            response_lower = response_text.lower()

            # 1a. Check for a perfect (case-insensitive) match first.
            perfect_matches = [cat for cat in categories if cat.lower() == response_lower]
            if perfect_matches:
                # Return the category with its original casing.
                return perfect_matches[0]

            # 1b. If no perfect match, check for an unambiguous substring match.
            substring_matches = [cat for cat in categories if response_lower in cat.lower()]
            
            if len(substring_matches) == 1:
                # Found exactly one unambiguous match.
                print(f"[*] Info: LLM returned '{response_text}'. Matched to unique category '{substring_matches[0]}' via substring.")
                return substring_matches[0]

            # 1c. If there is ambiguity (multiple substring matches), make a second LLM call.
            if len(substring_matches) > 1:
                return self._disambiguate_category(text, substring_matches, question_context)

            # 1d. If no matches of any kind were found.
            print(f"[!] Warning: LLM returned '{response_text}', which does not match any category. Returning 'Uncategorized'.")
            return "Uncategorized"
            
        except Exception as e:
            print(f"Error in classify_single: {e}")
            return "Classification Error"

    def classify_single_multiclass(self, text: str, categories: List[str], question_context: str = "") -> Dict[str, str]:
        """Classify a single text into multiple categories with robust parsing"""
        category_list = "\n".join(f"{i+1}. {cat}" for i, cat in enumerate(categories))
        
        # More explicit prompt format
        prompt = f"""You are a survey response classifier.
Survey question: {question_context}

Response: "{text}"

For each category below, indicate if the response applies (yes) or not (no).

Categories:
{category_list}

IMPORTANT: Reply with EXACTLY {len(categories)} answers in this format:
1. yes/no
2. yes/no
3. yes/no
(etc.)

Each line must start with a number followed by a period, then yes or no."""
        
        try:
            reply = self.send_chat([{"role": "system", "content": prompt}])
            answer = reply["message"]["content"].strip()
            
            # Parse numbered list format
            result = {}
            
            # Try to parse numbered format first (most reliable)
            numbered_pattern = r'(\d+)\.\s*(yes|no)'
            matches = re.findall(numbered_pattern, answer.lower())
            
            if matches:
                # Build result from numbered matches
                match_dict = {int(num): resp for num, resp in matches}
                for i, cat in enumerate(categories, 1):
                    if i in match_dict:
                        result[cat] = match_dict[i]
                    else:
                        result[cat] = "no"  # Default to no if missing
            else:
                # Fallback: try comma-separated or line-separated yes/no
                # Remove numbers, punctuation, extra whitespace
                clean_text = re.sub(r'[0-9\.\:\-\)\(]', ' ', answer.lower())
                # Find all yes/no occurrences
                responses = re.findall(r'\b(yes|no)\b', clean_text)
                
                for i, cat in enumerate(categories):
                    if i < len(responses):
                        result[cat] = responses[i]
                    else:
                        result[cat] = "no"
                        
            # Validate we have all categories
            for cat in categories:
                if cat not in result:
                    result[cat] = "no"
                    
            return result
            
        except Exception as e:
            print(f"Error in classify_single_multiclass: {e}")
            # Return all "no" as safe default
            return {cat: "no" for cat in categories}    

    def run_classification(
        self,
        df: pd.DataFrame,
        text_column: str,
        id_column: str,
        categories: Optional[List[str]] = None,
        multiclass: bool = False,
        n_samples: int = 100,
        question_context: str = "",
        category_model: Optional[str] = None,
        category_backend: Optional[str] = None
    ) -> Tuple[pd.DataFrame, List[str], Dict[str, Any]]:
        """Run full classification pipeline with error handling"""
        
        # Generate categories if needed
        if categories is None:
            try:
                cat_classifier = TextClassifier(
                    model_name=category_model or self.model_name,
                    backend=category_backend or self.backend
                )
                categories = cat_classifier.generate_categories(
                    df, text_column, n_samples, question_context
                )
                print(f"[*] Generated categories: {', '.join(categories)}")
            except Exception as e:
                print(f"Error generating categories: {e}")
                # Fallback to generic categories
                categories = ["Positive", "Negative", "Neutral", "Other"]
                print(f"[!] Using fallback categories: {', '.join(categories)}")
        else:
            print(f"[*] Using provided categories: {', '.join(categories)}")
        
        # Classify responses
        rows = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Classifying"):
            txt = row[text_column]
            # No need to check for empty/NaN - already filtered in api.py
            
            if multiclass:
                record = {
                    id_column: row[id_column],
                    text_column: txt,
                    **self.classify_single_multiclass(txt, categories, question_context)
                }
            else:
                record = {
                    id_column: row[id_column],
                    text_column: txt,
                    "category": self.classify_single(txt, categories, question_context)
                }
            rows.append(record)
        
        classified_df = pd.DataFrame(rows)
        
        # Calculate metrics
        metrics = {
            "total_rows": len(df),
            "classified_rows": len(classified_df),
            "multiclass": multiclass,
            "num_categories": len(categories)
        }
        
        if not multiclass:
            metrics["category_distribution"] = classified_df["category"].value_counts().to_dict()
        
        return classified_df, categories, metrics

# ===== setfit_classifier.py =====
# text_classifier/setfit_classifier.py
from typing import List, Dict, Any, Optional, Tuple, Union
import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime
from tqdm import tqdm

try:
    from setfit import SetFitModel, Trainer, TrainingArguments
    from sentence_transformers import SentenceTransformer
    from datasets import Dataset
    SETFIT_AVAILABLE = True
except ImportError:
    SETFIT_AVAILABLE = False
    # Define dummy classes if dependencies are not installed
    SetFitModel, Trainer, TrainingArguments, SentenceTransformer, Dataset = (None, None, None, None, None)


class SetFitClassifier:
    """SetFit-based text classifier for fast, few-shot learning, with multi-label support."""
    
    def __init__(
        self, 
        model_name: str = "sentence-transformers/paraphrase-mpnet-base-v2",
        device: str = None
    ):
        if not SETFIT_AVAILABLE:
            raise ImportError(
                "SetFit not installed. Run: pip install setfit sentence-transformers datasets"
            )
        
        self.model_name = model_name
        self.device = device
        self.model = None
        self.categories = None
        self.is_trained = False
        self.multiclass = False

    def train(
        self,
        texts: List[str],
        labels: Union[List[str], List[List[str]]], # Can be single or multi-label
        categories: List[str],
        multiclass: bool = False,
        validation_texts: Optional[List[str]] = None,
        validation_labels: Optional[Union[List[str], List[List[str]]]] = None,
        num_epochs: int = 1,
        batch_size: int = 16
    ) -> Dict[str, Any]:
        """Train SetFit model on labeled data (supports single and multi-label)."""
        print(f"[*] Training SetFit model with {len(texts)} samples...")
        
        self.categories = categories
        self.multiclass = multiclass
        
        # Convert labels to numeric format
        if multiclass:
            # Multi-label: multi-hot encode the labels
            label_to_id = {label: idx for idx, label in enumerate(categories)}
            numeric_labels = []
            for sample_labels in labels:
                multi_hot = np.zeros(len(categories), dtype=np.float32)
                for label in sample_labels:
                    if label in label_to_id:
                        multi_hot[label_to_id[label]] = 1.0
                numeric_labels.append(multi_hot)
        else:
            # Single-label: Filter out texts and labels that are not valid
            label_to_id = {label: idx for idx, label in enumerate(categories)}
            
            # Pair up texts and labels to filter them in unison
            paired_data = list(zip(texts, labels))
            valid_pairs = [(text, label) for text, label in paired_data if label in label_to_id]

            if len(valid_pairs) < len(paired_data):
                print(f"[!] Warning: Filtered out {len(paired_data) - len(valid_pairs)} samples with invalid labels (e.g., 'Uncategorized').")

            if not valid_pairs:
                print("[!] Error: No valid training data left after filtering. Aborting training.")
                return {"error": "No valid training data."}

            # Unzip back into separate lists, now guaranteed to be the same length
            texts, labels = zip(*valid_pairs)
            
            # Convert valid string labels to numeric IDs
            numeric_labels = [label_to_id[label] for label in labels]
        
        # Initialize model
        self.model = SetFitModel.from_pretrained(
            self.model_name,
            labels=list(range(len(categories))) if not multiclass else None,
            multi_target_strategy="one-vs-rest" if multiclass else None,
            device=self.device
        )
        
        # Create HuggingFace Dataset objects
        train_dataset = Dataset.from_dict({"text": list(texts), "label": numeric_labels})
        
        eval_dataset = None
        # (Validation logic for multiclass would need similar multi-hot encoding)

        args = TrainingArguments(
            batch_size=batch_size,
            num_epochs=num_epochs,
            report_to="none"
        )
        
        trainer = Trainer(
            model=self.model,
            args=args,
            train_dataset=train_dataset,
            column_mapping={"text": "text", "label": "label"}
        )
        
        trainer.train()
        self.is_trained = True
        
        metrics = {
            "num_training_samples": len(texts),
            "num_categories": len(categories),
            "multiclass": self.multiclass
        }
        return metrics            
    
    def predict_batch(
        self, 
        texts: List[str], 
        threshold: float = 0.5
    ) -> Union[List[str], List[List[str]]]:
        """Predict categories for multiple texts."""
        if not self.is_trained:
            raise ValueError("Model not trained. Call train() first.")
        
        if self.multiclass:
            # For multi-label, predict_proba gives probabilities for each class
            probas = self.model.predict_proba(texts)
            results = []
            for p_array in probas:
                # Get labels where probability is above the threshold
                predicted_labels = [
                    self.categories[i] for i, prob in enumerate(p_array) if prob >= threshold
                ]
                results.append(predicted_labels)
            return results
        else:
            # For single-label, predict gives the highest probability class
            predictions = self.model.predict(texts)
            return [self.categories[pred] for pred in predictions]
    
    def save(self, save_dir: Union[str, Path]):
        """Save trained model"""
        if not self.is_trained:
            raise ValueError("Model not trained. Nothing to save.")
        
        save_dir = Path(save_dir)
        save_dir.mkdir(exist_ok=True, parents=True)
        self.model.save_pretrained(str(save_dir))
        
        metadata = {
            "categories": self.categories,
            "model_name": self.model_name,
            "is_trained": self.is_trained,
            "multiclass": self.multiclass, # Save multiclass status
            "saved_at": datetime.now().isoformat()
        }
        with open(save_dir / "setfit_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        print(f"[*] SetFit model saved to: {save_dir}")
    
    def load(self, load_dir: Union[str, Path]):
        """Load trained model"""
        load_dir = Path(load_dir)
        if not (load_dir / "setfit_metadata.json").exists():
            raise ValueError(f"No SetFit model found in {load_dir}")
        
        with open(load_dir / "setfit_metadata.json") as f:
            metadata = json.load(f)
        
        self.categories = metadata["categories"]
        self.model_name = metadata["model_name"]
        self.is_trained = metadata["is_trained"]
        self.multiclass = metadata.get("multiclass", False) # Load multiclass status
        
        self.model = SetFitModel.from_pretrained(str(load_dir))
        print(f"[*] SetFit model loaded from: {load_dir}")


class HybridClassifier:
    """Hybrid classifier that uses LLM for training data and SetFit for bulk classification"""
    
    def __init__(
        self,
        llm_classifier,
        setfit_model_name: str = "sentence-transformers/paraphrase-mpnet-base-v2",
        confidence_threshold: float = 0.5, # For multiclass, this is the prediction threshold
        min_samples_per_category: int = 10,
        max_llm_samples: int = 200
    ):
        self.llm_classifier = llm_classifier
        self.setfit = SetFitClassifier(setfit_model_name)
        self.confidence_threshold = confidence_threshold
        self.min_samples_per_category = min_samples_per_category
        self.max_llm_samples = max_llm_samples
        self.training_data = []
        self.categories = None
        
    def collect_training_data(
        self,
        df: pd.DataFrame,
        text_column: str,
        id_column: str,
        categories: List[str],
        question_context: str = "",
        multiclass: bool = False
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Use LLM to classify initial samples for training"""
        print(f"[*] Collecting training data using LLM (max {self.max_llm_samples} samples)...")
        self.categories = categories
        
        sample_size = min(self.max_llm_samples, len(df))
        sample_df = df.sample(n=sample_size, random_state=42)
        
        rows = []
        for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc="LLM Classification"):
            text = row[text_column]
            record = {id_column: row[id_column], text_column: text}
            
            if multiclass:
                # Get {'cat1': 'yes', 'cat2': 'no'} from LLM
                labels_dict = self.llm_classifier.classify_single_multiclass(text, categories, question_context)
                # Convert to list of applicable categories: ['cat1']
                labels_list = [cat for cat, answer in labels_dict.items() if answer == 'yes']
                record['labels'] = labels_list # Store as a list
                # Also store the dict for the DataFrame
                record.update(labels_dict)

            else:
                label = self.llm_classifier.classify_single(text, categories, question_context)
                record['category'] = label
            
            rows.append(record)

        training_df = pd.DataFrame(rows)
        
        # Store for training
        if multiclass:
            self.training_data = list(zip(training_df[text_column], training_df['labels']))
        else:
            self.training_data = list(zip(training_df[text_column], training_df['category']))

        metrics = {"total_training_samples": len(training_df)}
        return training_df, metrics
    
    def train_setfit(self, multiclass: bool = False) -> Dict[str, Any]:
        """Train SetFit model on collected data"""
        if not self.training_data:
            raise ValueError("No training data collected. Run collect_training_data first.")
        
        texts, labels = zip(*self.training_data)
        
        # Train SetFit
        metrics = self.setfit.train(
            list(texts),
            list(labels),
            self.categories,
            multiclass=multiclass
        )
        return metrics
    
    def classify_hybrid(
        self,
        df: pd.DataFrame,
        text_column: str,
        id_column: str,
        question_context: str = "",
        multiclass: bool = False
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Classify using hybrid approach"""
        if not self.setfit.is_trained:
            raise ValueError("SetFit model not trained. Run train_setfit first.")
        
        print(f"[*] Running hybrid classification on {len(df)} samples...")
        
        # Get all SetFit predictions first in a batch
        setfit_predictions = self.setfit.predict_batch(
            df[text_column].tolist(), 
            threshold=self.confidence_threshold
        )
        
        results = []
        llm_count = 0
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Hybrid Classification"):
            text = row[text_column]
            record = {id_column: row[id_column], text_column: text}
            
            # For multiclass, we define "low confidence" as SetFit returning an empty list.
            # This means no category passed the confidence threshold.
            is_low_confidence = multiclass and not setfit_predictions[idx]

            if is_low_confidence:
                # Low confidence - use LLM
                llm_count += 1
                source = "llm"
                if multiclass:
                    labels_dict = self.llm_classifier.classify_single_multiclass(text, self.categories, question_context)
                    record.update(labels_dict)
                else: # Fallback for single-class (though confidence logic differs)
                    label = self.llm_classifier.classify_single(text, self.categories, question_context)
                    record['category'] = label
            else:
                # High confidence - use SetFit prediction
                source = "setfit"
                if multiclass:
                    # Convert list from SetFit to yes/no dictionary
                    labels_dict = {cat: ('yes' if cat in setfit_predictions[idx] else 'no') for cat in self.categories}
                    record.update(labels_dict)
                else:
                    record['category'] = setfit_predictions[idx]

            record['source'] = source
            results.append(record)
        
        results_df = pd.DataFrame(results)
        
        metrics = {
            "total_classified": len(results_df),
            "llm_classifications": llm_count,
            "setfit_classifications": len(df) - llm_count,
            "llm_percentage": (llm_count / len(df)) * 100 if len(df) > 0 else 0
        }
        
        return results_df, metrics


# ===== config.py =====
# text_classifier/config.py
"""
Configuration utilities for the text classifier system.
"""

import json
from pathlib import Path
from typing import Dict, Any, Union, List, Optional


def load_config(config_path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load configuration from a JSON file.
    
    Args:
        config_path: Path to the configuration JSON file
        
    Returns:
        Configuration dictionary with defaults applied
    """
    config_path = Path(config_path)
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    with open(config_path) as f:
        user_config = json.load(f)
    
    # Apply defaults
    return get_config_with_defaults(user_config)


def validate_config(config: Dict[str, Any]) -> None:
    """
    Validate that required configuration parameters are present and valid.
    
    Args:
        config: Configuration dictionary
        
    Raises:
        ValueError: If required parameters are missing or invalid
    """
    required = ["file_path", "text_column", "id_column"]
    missing = [key for key in required if key not in config]
    
    if missing:
        raise ValueError(f"Missing required config parameters: {missing}")
    
    # Validate file exists
    file_path = Path(config["file_path"])
    if not file_path.exists():
        raise FileNotFoundError(f"Input file not found: {file_path}")
    
    # Validate backends
    valid_backends = {"ollama", "openai"}
    backend_keys = ["classifier_backend", "category_backend", "judge_backend"]
    
    for key in backend_keys:
        if key in config and config[key] is not None:
            if config[key] not in valid_backends:
                raise ValueError(
                    f"Invalid {key}: '{config[key]}'. Must be one of: {valid_backends}"
                )


def merge_configs(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """
    Merge two configuration dictionaries, with override taking precedence.
    
    Args:
        base: Base configuration
        override: Configuration values to override
        
    Returns:
        Merged configuration
    """
    result = base.copy()
    result.update(override)
    return result


def parse_categories(config: Dict[str, Any]) -> Optional[List[str]]:
    """
    Parse categories from config, handling both string and list formats.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        List of categories or None
    """
    raw = config.get("categories")
    if raw is None:
        return None
    
    if isinstance(raw, str):
        categories = [c.strip() for c in raw.split(",") if c.strip()]
    elif isinstance(raw, list):
        categories = [str(c).strip() for c in raw if str(c).strip()]
    else:
        raise ValueError("'categories' must be a list or comma-separated string")
    
    return categories if categories else None


# Default configuration values
DEFAULT_CONFIG = {
    "classifier_model": "gemma3n:latest",
    "category_model": None,
    "judge_model": "gemma3n:latest",
    "classifier_backend": "ollama",
    "category_backend": None,
    "judge_backend": None,  # ADD THIS LINE - it was missing!
    "multiclass": False,
    "n_samples": 100,
    "question_context": "",
    "validation_samples": None,
    # Error handling options
    "max_retries": 3,
    "retry_delay": 1,
    "error_category": "Classification Error",
    "fallback_categories": ["Positive", "Negative", "Neutral", "Other"]
}


def get_config_with_defaults(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Apply default values to configuration.
    
    Args:
        config: User-provided configuration
        
    Returns:
        Configuration with defaults applied
    """
    # Start with defaults
    result = DEFAULT_CONFIG.copy()
    
    # Override with user config
    result.update(config)
    
    # Apply cascading defaults for backend settings
    # If category_backend not specified, use classifier_backend
    if result.get("category_backend") is None:
        result["category_backend"] = result["classifier_backend"]
    
    # If judge_backend not specified, use classifier_backend
    if result.get("judge_backend") is None:
        result["judge_backend"] = result["classifier_backend"]
    
    # If category_model not specified, use classifier_model
    if result.get("category_model") is None:
        result["category_model"] = result["classifier_model"]
    
    return result

# ===== models.py =====
# text_classifier/models.py
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path


@dataclass
class ClassificationRun:
    """Represents a single classification run with metadata"""
    run_id: str
    timestamp: datetime
    config: Dict[str, Any]
    input_file: Path
    output_file: Optional[Path] = None
    categories: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'run_id': self.run_id,
            'timestamp': self.timestamp.isoformat(),
            'config': self.config,
            'input_file': str(self.input_file),
            'output_file': str(self.output_file) if self.output_file else None,
            'categories': self.categories,
            'metrics': self.metrics
        }


@dataclass
class ValidationRun:
    """Represents a validation run"""
    validation_id: str
    classification_run_id: str
    timestamp: datetime
    config: Dict[str, Any]
    results_file: Optional[Path] = None
    metrics: Dict[str, float] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'validation_id': self.validation_id,
            'classification_run_id': self.classification_run_id,
            'timestamp': self.timestamp.isoformat(),
            'config': self.config,
            'results_file': str(self.results_file) if self.results_file else None,
            'metrics': self.metrics
        }

# ===== storage.py =====
# text_classifier/storage.py
import json
from pathlib import Path
from typing import Optional, List, Dict, Any, TYPE_CHECKING
import pandas as pd
from datetime import datetime

if TYPE_CHECKING:
    from .models import ClassificationRun


class RunStorage:
    """Manages storage and retrieval of classification/validation runs"""
    
    def __init__(self, base_dir: Path = Path("./runs")):
        self.base_dir = base_dir
        self.base_dir.mkdir(exist_ok=True)
        self.metadata_file = self.base_dir / "metadata.json"
        self._load_metadata()
    
    def _load_metadata(self):
        if self.metadata_file.exists():
            self.metadata = json.loads(self.metadata_file.read_text())
        else:
            self.metadata = {"classification_runs": {}, "validation_runs": {}}
    
    def _save_metadata(self):
        self.metadata_file.write_text(json.dumps(self.metadata, indent=2))
    
    def save_classification_run(self, run: 'ClassificationRun', df: pd.DataFrame):
        """Save classification run data and metadata"""
        # Create run directory
        run_dir = self.base_dir / f"classification_{run.run_id}"
        run_dir.mkdir(exist_ok=True)
        
        # Save data
        output_file = run_dir / f"classified_{run.run_id}.csv"
        df.to_csv(output_file, index=False)
        run.output_file = output_file
        
        # Save config
        config_file = run_dir / "config.json"
        config_file.write_text(json.dumps(run.config, indent=2))
        
        # Update metadata
        self.metadata["classification_runs"][run.run_id] = run.to_dict()
        self._save_metadata()
        
        return output_file
    
    def get_classification_run(self, run_id: str) -> Optional['ClassificationRun']:
        """Retrieve classification run metadata"""
        if run_id in self.metadata["classification_runs"]:
            data = self.metadata["classification_runs"][run_id]
            # Import at runtime
            from .models import ClassificationRun
            return ClassificationRun(
                run_id=data['run_id'],
                timestamp=datetime.fromisoformat(data['timestamp']),
                config=data['config'],
                input_file=Path(data['input_file']),
                output_file=Path(data['output_file']) if data['output_file'] else None,
                categories=data['categories'],
                metrics=data['metrics']
            )
        return None
    
    def load_classification_data(self, run_id: str) -> Optional[pd.DataFrame]:
        """Load classification results DataFrame"""
        run = self.get_classification_run(run_id)
        if run and run.output_file and run.output_file.exists():
            return pd.read_csv(run.output_file)
        return None
    
    def list_runs(self, run_type: str = "classification") -> List[Dict[str, Any]]:
        """List all runs of a given type"""
        key = f"{run_type}_runs"
        return list(self.metadata.get(key, {}).values())    

# ===== validator.py =====
# text_classifier/validator.py
from typing import Dict, Any, Optional, List, Tuple
import pandas as pd
from tqdm import tqdm
import re

from .classifier import TextClassifier


class ClassificationValidator:
    """Handles validation of classifications"""
    
    def __init__(self, model_name: str, backend: str = "ollama"):
        self.classifier = TextClassifier(model_name, backend)

    def validate_single_multiclass(
        self, 
        text: str, 
        predictions: Dict[str, str], 
        categories: List[str]
    ) -> Tuple[Dict[str, int], str]:
        """Validate multiclass predictions for a single text"""
        
        # Build the predictions summary
        pred_summary = "\n".join([
            f"- {cat}: {predictions.get(cat, 'no')}" 
            for cat in categories
        ])
        
        system_prompt = """You are evaluating multiclass text classifications.
For each category prediction, rate its accuracy from 1-5.
Format your response as:
Category scores:
- [Category name]: [1-5] - [brief reason]
Overall: [1-5] - [overall assessment]"""
        
        user_prompt = f"""Original text: "{text}"

Predictions:
{pred_summary}

For each category, evaluate if the yes/no prediction is accurate.
Consider:
1. Does a "yes" prediction correctly identify that the text relates to this category?
2. Does a "no" prediction correctly identify that the text does NOT relate to this category?
3. Are there any missed categories (should be "yes" but marked "no")?
4. Are there any false positives (marked "yes" but shouldn't be)?

Rate each category prediction 1-5 where:
1 = completely wrong
3 = partially correct  
5 = perfectly accurate"""
        
        try:
            result = self.classifier.send_chat([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ])['message']['content']
            
            # Parse individual category scores
            scores = self._parse_multiclass_scores(result, categories)
            
            return scores, result
            
        except Exception as e:
            print(f"Error in validate_single_multiclass: {e}")
            # Return middle scores as default
            return {cat: 3 for cat in categories}, f"Validation error: {str(e)}"
    
    def _parse_multiclass_scores(self, text: str, categories: List[str]) -> Dict[str, int]:
        """Parse scores for each category from validation response"""
        import re
        
        scores = {}
        
        # Try to find scores for each category
        for cat in categories:
            # Look for patterns like "CategoryName: 4" or "CategoryName: 4/5"
            # Make pattern flexible to handle variations
            escaped_cat = re.escape(cat)
            patterns = [
                rf'{escaped_cat}\s*:\s*(\d)',  # Category: 4
                rf'{escaped_cat}.*?:\s*(\d)',   # Category blah blah: 4
                rf'{escaped_cat}.*?(\d)\s*/',   # Category 4/5
                rf'{escaped_cat}.*?(\d)\s*out', # Category 4 out of 5
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    score = int(match.group(1))
                    if 1 <= score <= 5:
                        scores[cat] = score
                        break
            
            # Default to 3 if not found
            if cat not in scores:
                scores[cat] = 3
                
        # Try to find overall score
        overall_patterns = [
            r'[Oo]verall\s*:\s*(\d)',
            r'[Oo]verall.*?(\d)\s*/',
            r'[Aa]verage\s*:\s*(\d)',
        ]
        
        for pattern in overall_patterns:
            match = re.search(pattern, text)
            if match:
                score = int(match.group(1))
                if 1 <= score <= 5:
                    scores['_overall'] = score
                    break
                    
        return scores

    def validate_single(
        self, 
        text: str, 
        predicted_category: str, 
        categories: List[str]
    ) -> Tuple[int, str]:
        """Validate a single classification with robust score parsing"""
        categories_str = ", ".join(sorted(categories))
        
        system_prompt = """You are evaluating text classifications. 
Rate classification quality from 1 to 5 and explain.
Format your response as:
Score: [number]
Explanation: [your explanation]"""
        
        user_prompt = f"""Original text: \"{text}\"
Predicted category: \"{predicted_category}\"
Available categories: {categories_str}

Rate overall classification quality from 1 (very poor) to 5 (excellent)."""
        
        try:
            result = self.classifier.send_chat([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ])['message']['content']
            
            # More robust score parsing
            score = self._parse_score(result)
            
            if score is None:
                print(f"Warning: Could not parse score from: {result[:100]}...")
                score = 3  # Default middle score
                
            return score, result
            
        except Exception as e:
            print(f"Error in validate_single: {e}")
            return 3, f"Validation error: {str(e)}"

    def _parse_score(self, text: str) -> Optional[int]:
        """Parse score from text with multiple strategies"""
        # Strategy 1: Look for "Score: X" pattern
        score_match = re.search(r'[Ss]core\s*:\s*(\d)', text)
        if score_match:
            score = int(score_match.group(1))
            if 1 <= score <= 5:
                return score
        
        # Strategy 2: Look for "X/5" or "X out of 5" pattern
        out_of_five = re.search(r'(\d)\s*(?:/|out of)\s*5', text)
        if out_of_five:
            score = int(out_of_five.group(1))
            if 1 <= score <= 5:
                return score
        
        # Strategy 3: Find first standalone digit 1-5
        for match in re.finditer(r'\b([1-5])\b', text):
            return int(match.group(1))
        
        # Strategy 4: Look for written numbers
        word_to_num = {'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}
        for word, num in word_to_num.items():
            if word in text.lower():
                return num
        
        return None

    def validate_classification_run(
        self,
        df: pd.DataFrame,
        text_column: str,
        category_column: Optional[str] = None,  # Make optional for multiclass
        categories: List[str] = None,
        sample_size: Optional[int] = None,
        multiclass: bool = None  # Auto-detect if not specified
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Validate a classification run (both single and multiclass)"""
        
        # Auto-detect multiclass if not specified
        if multiclass is None:
            # Check if we have category columns (multiclass) or a single category column
            multiclass = all(cat in df.columns for cat in categories) if categories else False
        
        # Sample if requested
        if sample_size and sample_size < len(df):
            df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
        
        # Prepare results dataframe
        results_df = df.copy()
        
        if multiclass:
            # Multiclass validation
            results_df['quality_scores'] = None
            results_df['explanation'] = None
            results_df['average_score'] = None
            
            for idx, row in tqdm(df.iterrows(), total=len(df), desc="Validating multiclass"):
                # Get predictions for this row
                predictions = {cat: row.get(cat, 'no') for cat in categories}
                
                scores, explanation = self.validate_single_multiclass(
                    row[text_column],
                    predictions,
                    categories
                )
                
                # Store results
                results_df.at[idx, 'quality_scores'] = scores
                results_df.at[idx, 'explanation'] = explanation
                
                # Calculate average score for this row
                cat_scores = [s for c, s in scores.items() if c != '_overall']
                avg_score = sum(cat_scores) / len(cat_scores) if cat_scores else 3
                results_df.at[idx, 'average_score'] = avg_score
                
        else:
            # Single category validation (existing code)
            results_df['quality_score'] = None
            results_df['explanation'] = None
            
            for idx, row in tqdm(df.iterrows(), total=len(df), desc="Validating"):
                score, explanation = self.validate_single(
                    row[text_column],
                    row[category_column],
                    categories
                )
                results_df.at[idx, 'quality_score'] = score
                results_df.at[idx, 'explanation'] = explanation
        
        # Calculate metrics
        if multiclass:
            metrics = {
                "total_validated": len(results_df),
                "multiclass": True,
                "average_score": results_df['average_score'].mean(),
                "score_distribution": results_df['average_score'].value_counts().to_dict()
            }
            
            # Add per-category average scores
            category_scores = {}
            for cat in categories:
                cat_scores = [
                    scores.get(cat, 3) 
                    for scores in results_df['quality_scores'] 
                    if scores and cat in scores
                ]
                if cat_scores:
                    category_scores[cat] = sum(cat_scores) / len(cat_scores)
            metrics["category_scores"] = category_scores
            
        else:
            metrics = {
                "total_validated": len(results_df),
                "multiclass": False,
                "average_score": results_df['quality_score'].astype(float).mean(),
                "score_distribution": results_df['quality_score'].value_counts().to_dict()
            }
        
        return results_df, metrics

