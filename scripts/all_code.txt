# ===== __init__.py =====
# text_classifier/__init__.py
"""
Text Classification System

A flexible system for classifying open-text survey responses using LLMs.
"""

# Import main API functions
from .api import (
    classify_texts,
    validate_classification,
    load_classification_results,
    load_validation_results,
    compare_runs,
    list_all_runs,
    get_run_info,
    load_config,
    generate_categories_only,  
    load_saved_categories,
    classify_texts_hybrid,
    load_setfit_model      
)

# Import classes for advanced usage
from .classifier import TextClassifier
from .validator import ClassificationValidator
from .storage import RunStorage
from .models import ClassificationRun, ValidationRun

# Import SetFit classes
from .setfit_classifier import SetFitClassifier, HybridClassifier, SETFIT_AVAILABLE

__version__ = "0.1.0"

__all__ = [
    # Main API
    "classify_texts",
    "validate_classification",
    "load_classification_results",
    "load_validation_results",
    "compare_runs",
    "list_all_runs",
    "get_run_info",
    "load_config",
    "generate_categories_only",  
    "load_saved_categories",
    "classify_texts_hybrid",
    "load_setfit_model",
    
    # Classes
    "TextClassifier",
    "ClassificationValidator",
    "RunStorage",
    "ClassificationRun",
    "ValidationRun",
    
    # SetFit classes
    "SetFitClassifier",
    "HybridClassifier",
    "SETFIT_AVAILABLE"
]

# ===== api.py =====
# text_classifier/api.py
"""
High-level API for text classification system.
This module provides the main functions users should interact with.
"""

from pathlib import Path
from datetime import datetime
import uuid
import json
from typing import Dict, Any, Optional, List, Union, Tuple  # Added Union and Tuple
import pandas as pd

from .models import ClassificationRun, ValidationRun
from .storage import RunStorage
from .classifier import TextClassifier
from .validator import ClassificationValidator
from .setfit_classifier import SetFitClassifier, HybridClassifier, SETFIT_AVAILABLE
from .config import (
    load_config as _load_config,
    validate_config,
    get_config_with_defaults,
    parse_categories
)




def classify_texts(
    config: Dict[str, Any],
    run_id: Optional[str] = None,
    storage_dir: Path = Path("./runs")
) -> str:
    """
    Run text classification with proper storage.
    
    Args:
        config: Configuration dictionary with keys:
            - file_path: Path to input CSV
            - text_column: Column containing text
            - id_column: Column with unique IDs
            - categories: Optional list/string of categories
            - classifier_model: Model name (default: "gemma3n:latest")
            - classifier_backend: "ollama" or "openai" (default: "ollama")
            - category_backend: Backend for category generation (default: same as classifier_backend)
            - multiclass: Enable multi-label (default: False)
            - n_samples: Samples for category generation (default: 100)
            - question_context: Context for category generation
            - category_model: Model for generating categories
        run_id: Optional run ID (auto-generated if None)
        storage_dir: Directory to store runs
        
    Returns:
        run_id: Unique identifier for this classification run
    """
    # Apply defaults and validate
    config = get_config_with_defaults(config)
    validate_config(config)
    
    # Generate run ID if not provided
    if run_id is None:
        run_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    
    # Initialize storage
    storage = RunStorage(storage_dir)
    
    # Load data
    df = pd.read_csv(config["file_path"],keep_default_na=False)

    # Drop rows where text column is empty or has less than 1 character
    original_count = len(df)
    df = df[df[config["text_column"]].str.len() >= 1]
    dropped_count = original_count - len(df)
    
    if dropped_count > 0:
        print(f"[*] Dropped {dropped_count} rows with empty text")


    # Parse categories from config
    categories = parse_categories(config)
    
    # Initialize classifier
    classifier = TextClassifier(
        config["classifier_model"],
        config["classifier_backend"]
    )
    
    # Run classification
    classified_df, final_categories, metrics = classifier.run_classification(
        df=df,
        text_column=config["text_column"],
        id_column=config["id_column"],
        categories=categories,
        multiclass=config["multiclass"],
        n_samples=config["n_samples"],
        question_context=config["question_context"],
        category_model=config["category_model"],
        category_backend=config["category_backend"]
    )

    # Add dropped count to metrics
    metrics["dropped_empty_rows"] = dropped_count
    metrics["original_rows"] = original_count    
    
    # Create run record
    run = ClassificationRun(
        run_id=run_id,
        timestamp=datetime.now(),
        config=config,
        input_file=Path(config["file_path"]),
        categories=final_categories,
        metrics=metrics
    )
    
    # Save results
    output_file = storage.save_classification_run(run, classified_df)
    print(f"[*] Classification complete. Run ID: {run_id}")
    print(f"[*] Results saved to: {output_file}")
    
    return run_id


def validate_classification(
    classification_run_id: str,
    config: Optional[Dict[str, Any]] = None,
    sample_size: Optional[int] = None,
    storage_dir: Path = Path("./runs")
) -> str:
    """
    Validate a classification run using LLM-as-judge.
    Supports both single category and multiclass validation.
    
    Args:
        classification_run_id: ID of the classification run to validate
        config: Optional config to override validation settings
        sample_size: Number of samples to validate (None = all)
        storage_dir: Directory where runs are stored
        
    Returns:
        validation_id: Unique identifier for this validation run
    """
    # Initialize storage
    storage = RunStorage(storage_dir)
    
    # Load classification run
    class_run = storage.get_classification_run(classification_run_id)
    if not class_run:
        raise ValueError(f"Classification run {classification_run_id} not found")
    
    # Load classification data
    classified_df = storage.load_classification_data(classification_run_id)
    if classified_df is None:
        raise ValueError(f"Could not load data for run {classification_run_id}")
    
    # Use config from classification run if not provided
    if config is None:
        config = class_run.config.copy()
    
    # Override sample size if provided
    if sample_size is not None:
        config["validation_samples"] = sample_size
    
    # Initialize validator
    validator = ClassificationValidator(
        config["judge_model"],
        config["judge_backend"]  # Changed from config.get("backend", "ollama")
    )
    
    # Determine if multiclass
    multiclass = class_run.config.get("multiclass", False)
    
    # Run validation
    if multiclass:
        # For multiclass, we don't have a single category column
        validated_df, metrics = validator.validate_classification_run(
            df=classified_df,
            text_column=class_run.config["text_column"],
            category_column=None,
            categories=class_run.categories,
            sample_size=config.get("validation_samples"),
            multiclass=True
        )
    else:
        # Single category validation
        validated_df, metrics = validator.validate_classification_run(
            df=classified_df,
            text_column=class_run.config["text_column"],
            category_column="category",
            categories=class_run.categories,
            sample_size=config.get("validation_samples"),
            multiclass=False
        )
    
    # Generate validation ID
    val_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    
    # Save validation results
    val_dir = storage.base_dir / f"validation_{val_id}"
    val_dir.mkdir(exist_ok=True)
    
    output_file = val_dir / f"validation_{val_id}.csv"
    validated_df.to_csv(output_file, index=False)
    
    # Save validation metadata
    val_run = {
        "validation_id": val_id,
        "classification_run_id": classification_run_id,
        "timestamp": datetime.now().isoformat(),
        "config": config,
        "results_file": str(output_file),
        "metrics": metrics,
        "multiclass": multiclass
    }
    
    storage.metadata["validation_runs"][val_id] = val_run
    storage._save_metadata()
    
    print(f"[*] Validation complete. ID: {val_id}")
    print(f"[*] Average quality score: {metrics['average_score']:.2f}")
    
    if multiclass and "category_scores" in metrics:
        print(f"[*] Per-category scores:")
        for cat, score in metrics["category_scores"].items():
            print(f"    - {cat}: {score:.2f}")
    
    print(f"[*] Results saved to: {output_file}")
    
    return val_id

def load_classification_results(
    run_id: str,
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Load classification results for a given run.
    
    Args:
        run_id: Classification run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with classification results
    """
    storage = RunStorage(storage_dir)
    df = storage.load_classification_data(run_id)
    if df is None:
        raise ValueError(f"Could not load results for run {run_id}")
    return df


def load_validation_results(
    validation_id: str,
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Load validation results.
    
    Args:
        validation_id: Validation run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with validation results
    """
    storage = RunStorage(storage_dir)
    val_run = storage.metadata.get("validation_runs", {}).get(validation_id)
    if not val_run:
        raise ValueError(f"Validation run {validation_id} not found")
    
    results_file = Path(val_run["results_file"])
    if not results_file.exists():
        raise ValueError(f"Results file not found: {results_file}")
    
    return pd.read_csv(results_file)


def compare_runs(
    run_ids: List[str],
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Compare multiple classification runs.
    
    Args:
        run_ids: List of classification run IDs to compare
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with comparison metrics
    """
    storage = RunStorage(storage_dir)
    
    comparisons = []
    for run_id in run_ids:
        run = storage.get_classification_run(run_id)
        if run:
            comparison = {
                "run_id": run_id,
                "timestamp": run.timestamp,
                "model": run.config.get("classifier_model"),
                "backend": run.config.get("backend"),
                "multiclass": run.config.get("multiclass", False),
                "num_categories": len(run.categories),
                "total_rows": run.metrics.get("total_rows", 0),
                "classified_rows": run.metrics.get("classified_rows", 0)
            }
            
            # Add validation metrics if available
            validations = [v for v in storage.metadata.get("validation_runs", {}).values()
                          if v["classification_run_id"] == run_id]
            if validations:
                latest_val = max(validations, key=lambda x: x["timestamp"])
                comparison["validation_score"] = latest_val["metrics"]["average_score"]
                comparison["validated_samples"] = latest_val["metrics"]["total_validated"]
            
            comparisons.append(comparison)
    
    # Create comparison DataFrame
    comparison_df = pd.DataFrame(comparisons)
    
    print("\n=== Run Comparison ===")
    print(comparison_df.to_string())
    
    return comparison_df


def list_all_runs(
    run_type: str = "classification",
    storage_dir: Path = Path("./runs")
) -> List[Dict[str, Any]]:
    """
    List all runs of a given type.
    
    Args:
        run_type: "classification" or "validation"
        storage_dir: Directory where runs are stored
        
    Returns:
        List of run metadata dictionaries
    """
    storage = RunStorage(storage_dir)
    runs = storage.list_runs(run_type)
    
    # Sort by timestamp (newest first)
    runs.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
    
    return runs


def get_run_info(
    run_id: str,
    storage_dir: Path = Path("./runs")
) -> Dict[str, Any]:
    """
    Get detailed information about a specific run.
    
    Args:
        run_id: Classification or validation run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        Dictionary with run information
    """
    storage = RunStorage(storage_dir)
    
    # Check classification runs
    run = storage.get_classification_run(run_id)
    if run:
        return {
            "type": "classification",
            "run": run.to_dict(),
            "validations": [v for v in storage.metadata.get("validation_runs", {}).values()
                           if v["classification_run_id"] == run_id]
        }
    
    # Check validation runs
    val_run = storage.metadata.get("validation_runs", {}).get(run_id)
    if val_run:
        return {
            "type": "validation",
            "run": val_run,
            "classification_run": storage.get_classification_run(
                val_run["classification_run_id"]
            ).to_dict() if val_run.get("classification_run_id") else None
        }
    
    raise ValueError(f"Run {run_id} not found")

def generate_categories_only(
    config: Dict[str, Any],
    save_to_file: bool = True,
    storage_dir: Path = Path("./runs")
) -> List[str]:
    """
    Generate categories from data without running classification.
    
    Args:
        config: Configuration dictionary with keys:
            - file_path: Path to input CSV
            - text_column: Column containing text
            - n_samples: Number of samples for category generation (default: 100)
            - question_context: Context for category generation
            - category_model: Model for generating categories (default: classifier_model)
            - category_backend: Backend for category generation (default: classifier_backend)
        save_to_file: Whether to save categories to a JSON file
        storage_dir: Directory to save categories file
        
    Returns:
        List of generated categories
        
    Example:
        >>> config = {
        ...     "file_path": "survey.csv",
        ...     "text_column": "response",
        ...     "question_context": "What features would you like to see?"
        ... }
        >>> categories = generate_categories_only(config)
        >>> print(categories)
        ['Feature Request', 'Bug Report', 'Positive Feedback', ...]
    """
    # Apply defaults but only validate required fields for category generation
    config_with_defaults = get_config_with_defaults(config)
    
    # Minimal validation - we don't need id_column for category generation
    required = ["file_path", "text_column"]
    missing = [key for key in required if key not in config]
    if missing:
        raise ValueError(f"Missing required config parameters: {missing}")
    
    # Validate file exists
    file_path = Path(config_with_defaults["file_path"])  # Use config_with_defaults
    if not file_path.exists():
        raise FileNotFoundError(f"Input file not found: {file_path}")
    
    # Load data
    print(f"[*] Loading data from {file_path}")
    df = pd.read_csv(file_path, keep_default_na=False)
    
    # Drop empty rows
    original_count = len(df)
    df = df[df[config_with_defaults["text_column"]].str.len() >= 1]  # Use config_with_defaults
    dropped_count = original_count - len(df)
    
    if dropped_count > 0:
        print(f"[*] Dropped {dropped_count} rows with empty text")
    
    print(f"[*] Total rows available: {len(df)}")
    
    # Initialize classifier for category generation
    classifier = TextClassifier(
        config_with_defaults.get("category_model") or config_with_defaults["classifier_model"],
        config_with_defaults.get("category_backend") or config_with_defaults["classifier_backend"]
    )
    
    # Generate categories
    categories = classifier.generate_categories(
        df=df,
        text_column=config_with_defaults["text_column"],  # Use config_with_defaults
        n_samples=config_with_defaults.get("n_samples", 100),
        question_context=config_with_defaults.get("question_context", "")
    )
    
    print(f"\n[*] Generated {len(categories)} categories:")
    for i, cat in enumerate(categories, 1):
        print(f"    {i}. {cat}")
    
    # Save categories if requested
    if save_to_file:
        storage_dir = Path(storage_dir)
        storage_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        cat_file = storage_dir / f"categories_{timestamp}.json"
        
        save_data = {
            "categories": categories,
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "source_file": str(file_path),
                "text_column": config_with_defaults["text_column"],  # Use config_with_defaults
                "n_samples": config_with_defaults.get("n_samples", 100),
                "question_context": config_with_defaults.get("question_context", ""),
                "model": config_with_defaults.get("category_model") or config_with_defaults["classifier_model"],
                "backend": config_with_defaults.get("category_backend") or config_with_defaults["classifier_backend"],
                "total_rows": len(df),
                "dropped_empty_rows": dropped_count
            }
        }
        
        with open(cat_file, 'w') as f:
            json.dump(save_data, f, indent=2)
        
        print(f"\n[*] Categories saved to: {cat_file}")
    
    return categories

def load_saved_categories(
    categories_file: Union[str, Path]
) -> Tuple[List[str], Dict[str, Any]]:
    """
    Load categories from a previously saved categories file.
    
    Args:
        categories_file: Path to the categories JSON file
        
    Returns:
        Tuple of (categories list, metadata dict)
        
    Example:
        >>> categories, metadata = load_saved_categories("./runs/categories_20231230_143022.json")
        >>> print(f"Loaded {len(categories)} categories generated on {metadata['timestamp']}")
    """
    categories_file = Path(categories_file)
    
    if not categories_file.exists():
        raise FileNotFoundError(f"Categories file not found: {categories_file}")
    
    with open(categories_file) as f:
        data = json.load(f)
    
    return data["categories"], data.get("metadata", {})

# Convenience function for loading config from file
def load_config(config_path: str = "config.json") -> Dict[str, Any]:
    """Load configuration from JSON file."""
    return _load_config(config_path)

# Add to imports at the top of api.py
from .setfit_classifier import SetFitClassifier, HybridClassifier, SETFIT_AVAILABLE

# Add new function after the existing ones
def classify_texts_hybrid(
    config: Dict[str, Any],
    run_id: Optional[str] = None,
    storage_dir: Path = Path("./runs"),
    train_config: Optional[Dict[str, Any]] = None
) -> str:
    """
    Run text classification using hybrid LLM + SetFit approach.
    
    Args:
        config: Standard configuration dictionary plus:
            - use_setfit: Enable SetFit hybrid mode (default: True)
            - setfit_model: SetFit model name (default: "sentence-transformers/paraphrase-mpnet-base-v2")
            - confidence_threshold: Threshold for using SetFit vs LLM (default: 0.85)
            - max_llm_samples: Max samples to classify with LLM for training (default: 200)
            - min_samples_per_category: Min samples needed per category (default: 10)
        run_id: Optional run ID (auto-generated if None)
        storage_dir: Directory to store runs
        train_config: Optional config for SetFit training:
            - num_epochs: Training epochs (default: 1)
            - batch_size: Training batch size (default: 16)
            - validation_split: Validation split ratio (default: 0.2)
        
    Returns:
        run_id: Unique identifier for this classification run
    """
    if not SETFIT_AVAILABLE:
        raise ImportError("SetFit not available. Run: pip install setfit sentence-transformers")
    
    # Apply defaults and validate
    config = get_config_with_defaults(config)
    validate_config(config)
    
    # SetFit specific defaults
    setfit_defaults = {
        "use_setfit": True,
        "setfit_model": "sentence-transformers/paraphrase-mpnet-base-v2",
        "confidence_threshold": 0.85,
        "max_llm_samples": 200,
        "min_samples_per_category": 10
    }
    
    for key, value in setfit_defaults.items():
        if key not in config:
            config[key] = value
    
    # Generate run ID if not provided
    if run_id is None:
        run_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    
    # Initialize storage
    storage = RunStorage(storage_dir)
    run_dir = storage.base_dir / f"classification_{run_id}"
    run_dir.mkdir(exist_ok=True)
    
    # Load and clean data
    df = pd.read_csv(config["file_path"], keep_default_na=False)
    original_count = len(df)
    df = df[df[config["text_column"]].str.len() >= 1]
    dropped_count = original_count - len(df)
    
    if dropped_count > 0:
        print(f"[*] Dropped {dropped_count} rows with empty text")
    
    # Parse categories
    categories = parse_categories(config)
    
    # Initialize LLM classifier
    llm_classifier = TextClassifier(
        config["classifier_model"],
        config["classifier_backend"]
    )
    
    # Generate categories if needed
    if categories is None:
        cat_classifier = TextClassifier(
            model_name=config.get("category_model") or config["classifier_model"],
            backend=config.get("category_backend") or config["classifier_backend"]
        )
        categories = cat_classifier.generate_categories(
            df, 
            config["text_column"], 
            config["n_samples"], 
            config["question_context"]
        )
        print(f"[*] Generated categories: {', '.join(categories)}")
    else:
        print(f"[*] Using provided categories: {', '.join(categories)}")
    
    # Initialize hybrid classifier
    hybrid = HybridClassifier(
        llm_classifier,
        config["setfit_model"],
        config["confidence_threshold"],
        config["min_samples_per_category"],
        config["max_llm_samples"]
    )
    
    # Collect training data
    print("\n=== Phase 1: Collecting training data with LLM ===")
    training_df, training_metrics = hybrid.collect_training_data(
        df.sample(frac=1, random_state=42),  # Shuffle for better sampling
        config["text_column"],
        config["id_column"],
        categories,
        config["question_context"]
    )
    
    # Save training data
    training_file = run_dir / "training_data.csv"
    training_df.to_csv(training_file, index=False)
    
    # Train SetFit
    print("\n=== Phase 2: Training SetFit model ===")
    train_cfg = train_config or {}
    setfit_metrics = hybrid.train_setfit(
        validation_split=train_cfg.get("validation_split", 0.2)
    )
    
    # Save SetFit model
    setfit_dir = run_dir / "setfit_model"
    hybrid.setfit.save(setfit_dir)
    
    # Classify all data
    print("\n=== Phase 3: Hybrid classification ===")
    classified_df, classification_metrics = hybrid.classify_hybrid(
        df,
        config["text_column"],
        config["id_column"],
        config["question_context"],
        use_active_learning=True
    )
    
    # Combine all metrics
    metrics = {
        "total_rows": len(df),
        "dropped_empty_rows": dropped_count,
        "original_rows": original_count,
        "training_metrics": training_metrics,
        "setfit_metrics": setfit_metrics,
        "classification_metrics": classification_metrics,
        "hybrid_mode": True,
        "num_categories": len(categories)
    }
    
    # Create run record
    run = ClassificationRun(
        run_id=run_id,
        timestamp=datetime.now(),
        config=config,
        input_file=Path(config["file_path"]),
        categories=categories,
        metrics=metrics
    )
    
    # Save results
    output_file = storage.save_classification_run(run, classified_df)
    
    print(f"\n[*] Hybrid classification complete. Run ID: {run_id}")
    print(f"[*] Results saved to: {output_file}")
    print(f"[*] LLM used for: {classification_metrics['llm_percentage']:.1f}% of classifications")
    
    return run_id


def load_setfit_model(
    run_id: str,
    storage_dir: Path = Path("./runs")
) -> SetFitClassifier:
    """
    Load a trained SetFit model from a previous run.
    
    Args:
        run_id: Classification run ID that used SetFit
        storage_dir: Directory where runs are stored
        
    Returns:
        Loaded SetFitClassifier instance
    """
    run_dir = storage_dir / f"classification_{run_id}" / "setfit_model"
    
    if not run_dir.exists():
        raise ValueError(f"No SetFit model found for run {run_id}")
    
    classifier = SetFitClassifier()
    classifier.load(run_dir)
    
    return classifier

# ===== classifier.py =====
# text_classifier/classifier.py
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
from tqdm import tqdm
import os
import time
import re

try:
    import ollama
except ImportError:
    ollama = None
    
try:
    import openai
except ImportError:
    openai = None


class TextClassifier:
    def __init__(self, model_name: str, backend: str = "ollama", max_retries: int = 3):
        self.model_name = model_name
        self.backend = backend
        self.max_retries = max_retries

        if backend == "openai":
            if openai is None:
                raise ImportError("OpenAI package not installed. Run: pip install openai")
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("OPENAI_API_KEY environment variable not set")
            self.client = openai.OpenAI()
        elif backend == "ollama":
            if ollama is None:
                raise ImportError("Ollama package not installed. Run: pip install ollama")
    
    def send_chat(self, messages: List[Dict[str, Any]], retry_count: int = 0) -> Dict[str, Any]:
        """Send chat request to LLM with retry logic"""
        try:
            if self.backend == "openai":
                try:
                    resp = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=messages,
                    )
                    content = resp.choices[0].message.content
                    return {"message": {"content": content}}
                except openai.RateLimitError as e:
                    if retry_count < self.max_retries:
                        wait_time = 2 ** retry_count  # Exponential backoff
                        print(f"Rate limit hit, waiting {wait_time}s...")
                        time.sleep(wait_time)
                        return self.send_chat(messages, retry_count + 1)
                    raise
                except openai.APIError as e:
                    if retry_count < self.max_retries:
                        print(f"API error, retrying... ({retry_count + 1}/{self.max_retries})")
                        time.sleep(1)
                        return self.send_chat(messages, retry_count + 1)
                    raise
            else:  # ollama
                try:
                    return ollama.chat(model=self.model_name, messages=messages)
                except Exception as e:
                    if retry_count < self.max_retries:
                        print(f"Ollama error, retrying... ({retry_count + 1}/{self.max_retries})")
                        time.sleep(1)
                        return self.send_chat(messages, retry_count + 1)
                    raise
                    
        except Exception as e:
            print(f"Failed after {self.max_retries} retries: {e}")
            raise
    
    def generate_categories(
        self,
        df: pd.DataFrame,
        text_column: str,
        n_samples: int = 100,
        question_context: str = ""
    ) -> List[str]:
        """Generate categories from sample responses with robust parsing"""
        print(f"[*] Sampling {n_samples} responses to generate categories...")
        sample = df[text_column].dropna().sample(n=min(n_samples, len(df)))
        sample_text = "\n\n".join(sample)
        
        # More explicit prompt format
        prompt = f"""The following are sample survey responses to the survey question: {question_context}

{sample_text}

Based on these responses, generate 5-15 mutually exclusive categories that summarise the main themes.

IMPORTANT: Format your response as a numbered list, one category per line:
1. First Category
2. Second Category
3. Third Category
...

Include a "Don't know/Uncertain" or "Other" category if appropriate.
Each category should be concise (2-5 words) and clearly distinct from others."""
        
        try:
            reply = self.send_chat([{"role": "system", "content": prompt}])
            text = reply["message"]["content"].strip()
            
            # Parse categories with multiple strategies
            categories = self._parse_categories(text)
            
            if not categories:
                print("[!] Warning: Could not parse categories from LLM response")
                # Fallback to generic categories
                categories = ["Positive", "Negative", "Neutral", "Other"]
            
            # Validate and clean categories
            categories = self._validate_categories(categories)
            
            return categories
            
        except Exception as e:
            print(f"[!] Error generating categories: {e}")
            # Return sensible defaults
            return ["Positive", "Negative", "Neutral", "Other"]

    def _validate_categories(self, categories: List[str]) -> List[str]:
        """Validate and clean the parsed categories"""
        cleaned = []
        seen = set()
        
        for cat in categories:
            # Remove any remaining numbering or bullets
            cat = re.sub(r'^\d+[\.\)]\s*', '', cat)
            cat = re.sub(r'^[-*•]\s*', '', cat)
            
            # Remove quotes
            cat = cat.strip('"\'')
            
            # Remove trailing punctuation
            cat = cat.rstrip('.,;:')
            
            # Normalize whitespace
            cat = ' '.join(cat.split())
            
            # Skip if too short or too long
            if len(cat) < 2 or len(cat) > 100:
                continue
            
            # Skip duplicates (case-insensitive)
            cat_lower = cat.lower()
            if cat_lower in seen:
                continue
            seen.add(cat_lower)
            
            cleaned.append(cat)
        
        # Ensure we have at least some categories
        if len(cleaned) < 3:
            print(f"[!] Warning: Only {len(cleaned)} valid categories found")
            # Add some generic ones if needed
            generic = ["Other", "Uncertain", "Mixed"]
            for g in generic:
                if g.lower() not in seen:
                    cleaned.append(g)
                    if len(cleaned) >= 5:
                        break
        
        # Cap at reasonable number
        if len(cleaned) > 15:
            print(f"[!] Warning: {len(cleaned)} categories found, capping at 15")
            cleaned = cleaned[:15]
        
        return cleaned

    def _parse_categories(self, text: str) -> List[str]:
        """Parse categories from LLM response using multiple strategies"""
        categories = []
        
        # Strategy 1: Parse numbered list (1. Category, 2. Category, etc.)
        numbered_pattern = r'^\s*\d+[\.\)]\s*(.+)$'
        lines = text.strip().split('\n')
        
        for line in lines:
            match = re.match(numbered_pattern, line.strip())
            if match:
                categories.append(match.group(1).strip())
        
        if categories:
            return categories
        
        # Strategy 2: Parse bullet points (- Category, * Category, • Category)
        bullet_pattern = r'^\s*[-*•]\s*(.+)$'
        for line in lines:
            match = re.match(bullet_pattern, line.strip())
            if match:
                categories.append(match.group(1).strip())
        
        if categories:
            return categories
        
        # Strategy 3: Parse comma-separated list
        # Look for a line that contains multiple comma-separated items
        for line in lines:
            if ',' in line:
                # Clean up common prefixes
                line = re.sub(r'^(Categories:|The categories are:|Here are the categories:)', '', line, flags=re.IGNORECASE)
                line = line.strip()
                
                # Split by comma and clean each item
                potential_categories = [
                    item.strip().strip('"\'') 
                    for item in line.split(',') 
                    if item.strip()
                ]
                
                # Validate these look like categories (not too long)
                if all(len(cat) < 50 for cat in potential_categories) and len(potential_categories) >= 3:
                    categories = potential_categories
                    break
        
        if categories:
            return categories
        
        # Strategy 4: Each non-empty line as a category (if reasonable number)
        # Filter out common non-category lines
        exclude_patterns = [
            r'^(here|these|the|based on|categories|following)',
            r'^(include|should|must|note)',
            r'^\d+$',  # Just numbers
            r'^[a-z]$',  # Single lowercase letters
        ]
        
        potential_categories = []
        for line in lines:
            line = line.strip()
            if line and len(line) < 50:  # Reasonable length for category
                # Check if line should be excluded
                if not any(re.match(pattern, line.lower()) for pattern in exclude_patterns):
                    potential_categories.append(line)
        
        # If we got a reasonable number of categories this way
        if 3 <= len(potential_categories) <= 20:
            return potential_categories
        
        return categories

    def classify_single(self, text: str, categories: List[str], question_context: str = "") -> str:
        """Classify a single text with error handling"""
        prompt = (
            "You are a survey response classifier.\n"
            f"Survey question: {question_context}\n\n"
            f"Response:\n\"{text}\"\n\n"
            f"Choose the best category among:\n{', '.join(categories)}\n\n"
            "Return the category name only."
        )
        
        try:
            reply = self.send_chat([{"role": "system", "content": prompt}])
            response = reply["message"]["content"].strip()
            
            # Validate response is in categories
            if response not in categories:
                # Try case-insensitive match
                for cat in categories:
                    if cat.lower() == response.lower():
                        return cat
                
                # Try regex matching - check if response is contained in exactly one category
                response_lower = response.lower()
                matches = []
                
                for cat in categories:
                    if response_lower in cat.lower():
                        matches.append(cat)
                
                if len(matches) == 1:
                    # Found in exactly one category
                    print(f"Warning: LLM returned '{response}' - matched to category '{matches[0]}' via substring match.")
                    return matches[0]
                else:
                    # Not found or found in multiple categories
                    if len(matches) > 1:
                        print(f"Warning: LLM returned '{response}' which matches multiple categories: {matches}. Returning 'Uncategorized'.")
                    else:
                        print(f"Warning: LLM returned '{response}' which doesn't match any category. Returning 'Uncategorized'.")
                    return "Uncategorized"
            
            return response
            
        except Exception as e:
            print(f"Error in classify_single: {e}")
            # Return a default category or raise depending on your needs
            return "Classification Error"    

    def classify_single_multiclass(self, text: str, categories: List[str], question_context: str = "") -> Dict[str, str]:
        """Classify a single text into multiple categories with robust parsing"""
        category_list = "\n".join(f"{i+1}. {cat}" for i, cat in enumerate(categories))
        
        # More explicit prompt format
        prompt = f"""You are a survey response classifier.
Survey question: {question_context}

Response: "{text}"

For each category below, indicate if the response applies (yes) or not (no).

Categories:
{category_list}

IMPORTANT: Reply with EXACTLY {len(categories)} answers in this format:
1. yes/no
2. yes/no
3. yes/no
(etc.)

Each line must start with a number followed by a period, then yes or no."""
        
        try:
            reply = self.send_chat([{"role": "system", "content": prompt}])
            answer = reply["message"]["content"].strip()
            
            # Parse numbered list format
            result = {}
            
            # Try to parse numbered format first (most reliable)
            numbered_pattern = r'(\d+)\.\s*(yes|no)'
            matches = re.findall(numbered_pattern, answer.lower())
            
            if matches:
                # Build result from numbered matches
                match_dict = {int(num): resp for num, resp in matches}
                for i, cat in enumerate(categories, 1):
                    if i in match_dict:
                        result[cat] = match_dict[i]
                    else:
                        result[cat] = "no"  # Default to no if missing
            else:
                # Fallback: try comma-separated or line-separated yes/no
                # Remove numbers, punctuation, extra whitespace
                clean_text = re.sub(r'[0-9\.\:\-\)\(]', ' ', answer.lower())
                # Find all yes/no occurrences
                responses = re.findall(r'\b(yes|no)\b', clean_text)
                
                for i, cat in enumerate(categories):
                    if i < len(responses):
                        result[cat] = responses[i]
                    else:
                        result[cat] = "no"
                        
            # Validate we have all categories
            for cat in categories:
                if cat not in result:
                    result[cat] = "no"
                    
            return result
            
        except Exception as e:
            print(f"Error in classify_single_multiclass: {e}")
            # Return all "no" as safe default
            return {cat: "no" for cat in categories}    

    def run_classification(
        self,
        df: pd.DataFrame,
        text_column: str,
        id_column: str,
        categories: Optional[List[str]] = None,
        multiclass: bool = False,
        n_samples: int = 100,
        question_context: str = "",
        category_model: Optional[str] = None,
        category_backend: Optional[str] = None
    ) -> Tuple[pd.DataFrame, List[str], Dict[str, Any]]:
        """Run full classification pipeline with error handling"""
        
        # Generate categories if needed
        if categories is None:
            try:
                cat_classifier = TextClassifier(
                    model_name=category_model or self.model_name,
                    backend=category_backend or self.backend
                )
                categories = cat_classifier.generate_categories(
                    df, text_column, n_samples, question_context
                )
                print(f"[*] Generated categories: {', '.join(categories)}")
            except Exception as e:
                print(f"Error generating categories: {e}")
                # Fallback to generic categories
                categories = ["Positive", "Negative", "Neutral", "Other"]
                print(f"[!] Using fallback categories: {', '.join(categories)}")
        else:
            print(f"[*] Using provided categories: {', '.join(categories)}")
        
        # Classify responses
        rows = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Classifying"):
            txt = row[text_column]
            # No need to check for empty/NaN - already filtered in api.py
            
            if multiclass:
                record = {
                    id_column: row[id_column],
                    text_column: txt,
                    **self.classify_single_multiclass(txt, categories, question_context)
                }
            else:
                record = {
                    id_column: row[id_column],
                    text_column: txt,
                    "category": self.classify_single(txt, categories, question_context)
                }
            rows.append(record)
        
        classified_df = pd.DataFrame(rows)
        
        # Calculate metrics
        metrics = {
            "total_rows": len(df),
            "classified_rows": len(classified_df),
            "multiclass": multiclass,
            "num_categories": len(categories)
        }
        
        if not multiclass:
            metrics["category_distribution"] = classified_df["category"].value_counts().to_dict()
        
        return classified_df, categories, metrics

# ===== setfit_classifier.py =====
# text_classifier/setfit_classifier.py
from typing import List, Dict, Any, Optional, Tuple, Union
import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime
from tqdm import tqdm

try:
    from setfit import SetFitModel
    from sentence_transformers import SentenceTransformer
    SETFIT_AVAILABLE = True
except ImportError:
    SETFIT_AVAILABLE = False
    SetFitModel = None
    SentenceTransformer = None


class SetFitClassifier:
    """SetFit-based text classifier for fast, few-shot learning"""
    
    def __init__(
        self, 
        model_name: str = "sentence-transformers/paraphrase-mpnet-base-v2",
        device: str = None
    ):
        if not SETFIT_AVAILABLE:
            raise ImportError(
                "SetFit not installed. Run: pip install setfit sentence-transformers"
            )
        
        self.model_name = model_name
        self.device = device
        self.model = None
        self.categories = None
        self.is_trained = False

    def train(
        self,
        texts: List[str],
        labels: List[str],
        categories: List[str],
        validation_texts: Optional[List[str]] = None,
        validation_labels: Optional[List[str]] = None,
        num_epochs: int = 1,
        batch_size: int = 16
    ) -> Dict[str, Any]:
        """Train SetFit model on labeled data"""
        print(f"[*] Training SetFit model with {len(texts)} samples...")
        
        # Store categories for later use
        self.categories = categories
        
        # Convert string labels to numeric
        label_to_id = {label: idx for idx, label in enumerate(categories)}
        numeric_labels = [label_to_id[label] for label in labels]
        
        # Initialize model
        from setfit import SetFitModel, Trainer, TrainingArguments
        from datasets import Dataset
        
        self.model = SetFitModel.from_pretrained(
            self.model_name,
            labels=list(range(len(categories))),
            device=self.device
        )
        
        # Create HuggingFace Dataset objects
        train_dataset = Dataset.from_dict({
            "text": texts,
            "label": numeric_labels
        })
        
        eval_dataset = None
        if validation_texts and validation_labels:
            val_numeric_labels = [label_to_id[label] for label in validation_labels]
            eval_dataset = Dataset.from_dict({
                "text": validation_texts,
                "label": val_numeric_labels
            })
        
        # Create training arguments
        args = TrainingArguments(
            batch_size=batch_size,
            num_epochs=num_epochs,
            eval_strategy="epoch" if eval_dataset else "no",
            save_strategy="no",
            load_best_model_at_end=False,
            report_to="none"  # Disable wandb/tensorboard
        )
        
        # Create trainer
        trainer = Trainer(
            model=self.model,
            args=args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            column_mapping={"text": "text", "label": "label"}  # Explicit mapping
        )
        
        # Train
        trainer.train()
        
        self.is_trained = True
        
        # Calculate training metrics
        metrics = {
            "num_training_samples": len(texts),
            "num_categories": len(categories),
            "model_name": self.model_name,
            "num_epochs": num_epochs,
            "categories_with_samples": len(set(labels))
        }
        
        # If validation data provided, calculate accuracy
        if validation_texts and validation_labels:
            predictions = self.predict_batch(validation_texts)
            accuracy = sum(p == l for p, l in zip(predictions, validation_labels)) / len(validation_labels)
            metrics["validation_accuracy"] = accuracy
            
        return metrics            
    
    def predict(self, text: str, return_proba: bool = False) -> Union[str, Tuple[str, float]]:
        """Predict category for a single text"""
        if not self.is_trained:
            raise ValueError("Model not trained. Call train() first.")
        
        # Get prediction
        prediction = self.model.predict([text])[0]
        
        if return_proba:
            # Get probabilities
            proba = self.model.predict_proba([text])[0]
            confidence = float(max(proba))
            return self.categories[prediction], confidence
        
        return self.categories[prediction]
    
    def predict_batch(
        self, 
        texts: List[str], 
        return_proba: bool = False
    ) -> Union[List[str], List[Tuple[str, float]]]:
        """Predict categories for multiple texts"""
        if not self.is_trained:
            raise ValueError("Model not trained. Call train() first.")
        
        predictions = self.model.predict(texts)
        
        if return_proba:
            probas = self.model.predict_proba(texts)
            results = []
            for pred, proba in zip(predictions, probas):
                confidence = float(max(proba))
                results.append((self.categories[pred], confidence))
            return results
        
        return [self.categories[pred] for pred in predictions]
    
    def save(self, save_dir: Union[str, Path]):
        """Save trained model"""
        if not self.is_trained:
            raise ValueError("Model not trained. Nothing to save.")
        
        save_dir = Path(save_dir)
        save_dir.mkdir(exist_ok=True, parents=True)
        
        # Save model
        self.model.save_pretrained(str(save_dir))
        
        # Save metadata
        metadata = {
            "categories": self.categories,
            "model_name": self.model_name,
            "is_trained": self.is_trained,
            "saved_at": datetime.now().isoformat()
        }
        
        with open(save_dir / "setfit_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
            
        print(f"[*] SetFit model saved to: {save_dir}")
    
    def load(self, load_dir: Union[str, Path]):
        """Load trained model"""
        load_dir = Path(load_dir)
        
        if not (load_dir / "setfit_metadata.json").exists():
            raise ValueError(f"No SetFit model found in {load_dir}")
        
        # Load metadata
        with open(load_dir / "setfit_metadata.json") as f:
            metadata = json.load(f)
        
        self.categories = metadata["categories"]
        self.model_name = metadata["model_name"]
        self.is_trained = metadata["is_trained"]
        
        # Load model
        self.model = SetFitModel.from_pretrained(str(load_dir))
        
        print(f"[*] SetFit model loaded from: {load_dir}")


class HybridClassifier:
    """Hybrid classifier that uses LLM for training data and SetFit for bulk classification"""
    
    def __init__(
        self,
        llm_classifier,  # TextClassifier instance
        setfit_model_name: str = "sentence-transformers/paraphrase-mpnet-base-v2",
        confidence_threshold: float = 0.85,
        min_samples_per_category: int = 10,
        max_llm_samples: int = 200
    ):
        self.llm_classifier = llm_classifier
        self.setfit = SetFitClassifier(setfit_model_name)
        self.confidence_threshold = confidence_threshold
        self.min_samples_per_category = min_samples_per_category
        self.max_llm_samples = max_llm_samples
        self.training_data = []
        self.categories = None
        
    def collect_training_data(
        self,
        df: pd.DataFrame,
        text_column: str,
        id_column: str,
        categories: List[str],
        question_context: str = ""
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Use LLM to classify initial samples for training"""
        print(f"[*] Collecting training data using LLM (max {self.max_llm_samples} samples)...")
        
        self.categories = categories
        
        # Sample data for training
        sample_size = min(self.max_llm_samples, len(df))
        sample_df = df.sample(n=sample_size, random_state=42)
        
        texts = []
        labels = []
        ids = []
        
        for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc="LLM Classification"):
            text = row[text_column]
            label = self.llm_classifier.classify_single(text, categories, question_context)
            
            texts.append(text)
            labels.append(label)
            ids.append(row[id_column])
        
        # Create training dataframe
        training_df = pd.DataFrame({
            id_column: ids,
            text_column: texts,
            'category': labels
        })
        
        # Store for later use
        self.training_data = list(zip(texts, labels))
        
        # Check if we have enough samples per category
        category_counts = training_df['category'].value_counts()
        metrics = {
            "total_training_samples": len(training_df),
            "category_distribution": category_counts.to_dict(),
            "categories_with_insufficient_samples": [
                cat for cat in categories 
                if category_counts.get(cat, 0) < self.min_samples_per_category
            ]
        }
        
        return training_df, metrics
    
    def train_setfit(self, validation_split: float = 0.2) -> Dict[str, Any]:
        """Train SetFit model on collected data"""
        if not self.training_data:
            raise ValueError("No training data collected. Run collect_training_data first.")
        
        texts, labels = zip(*self.training_data)
        texts, labels = list(texts), list(labels)
        
        # Filter out 'Uncategorized' samples
        filtered_data = [(t, l) for t, l in zip(texts, labels) if l != 'Uncategorized']
        
        if not filtered_data:
            raise ValueError("No valid training data after filtering 'Uncategorized' samples")
        
        if len(filtered_data) < len(texts):
            print(f"[!] Filtered out {len(texts) - len(filtered_data)} 'Uncategorized' samples")
        
        texts, labels = zip(*filtered_data)
        texts, labels = list(texts), list(labels)
        
        # Check if we still have all categories represented
        unique_labels = set(labels)
        missing_categories = set(self.categories) - unique_labels
        if missing_categories:
            print(f"[!] Warning: No training samples for categories: {missing_categories}")
    
        # Split for validation
        if validation_split > 0:
            split_idx = int(len(texts) * (1 - validation_split))
            train_texts, val_texts = texts[:split_idx], texts[split_idx:]
            train_labels, val_labels = labels[:split_idx], labels[split_idx:]
        else:
            train_texts, val_texts = texts, None
            train_labels, val_labels = labels, None
        
        # Train SetFit
        metrics = self.setfit.train(
            train_texts,
            train_labels,
            self.categories,
            val_texts,
            val_labels
        )
        
        return metrics
    
    def classify_hybrid(
        self,
        df: pd.DataFrame,
        text_column: str,
        id_column: str,
        question_context: str = "",
        use_active_learning: bool = True
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Classify using hybrid approach"""
        if not self.setfit.is_trained:
            raise ValueError("SetFit model not trained. Run train_setfit first.")
        
        print(f"[*] Running hybrid classification on {len(df)} samples...")
        
        results = []
        llm_count = 0
        setfit_count = 0
        
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Hybrid Classification"):
            text = row[text_column]
            
            if use_active_learning:
                # Get prediction with confidence
                category, confidence = self.setfit.predict(text, return_proba=True)
                
                if confidence < self.confidence_threshold:
                    # Low confidence - use LLM
                    category = self.llm_classifier.classify_single(
                        text, self.categories, question_context
                    )
                    llm_count += 1
                    source = "llm"
                else:
                    # High confidence - use SetFit prediction
                    setfit_count += 1
                    source = "setfit"
            else:
                # Just use SetFit
                category = self.setfit.predict(text)
                setfit_count += 1
                source = "setfit"
                confidence = None
            
            result = {
                id_column: row[id_column],
                text_column: text,
                'category': category,
                'source': source
            }
            
            if confidence is not None:
                result['confidence'] = confidence
                
            results.append(result)
        
        results_df = pd.DataFrame(results)
        
        metrics = {
            "total_classified": len(results_df),
            "llm_classifications": llm_count,
            "setfit_classifications": setfit_count,
            "llm_percentage": (llm_count / len(results_df)) * 100 if len(results_df) > 0 else 0,
            "category_distribution": results_df['category'].value_counts().to_dict()
        }
        
        if 'confidence' in results_df.columns:
            metrics["average_confidence"] = results_df['confidence'].mean()
            metrics["low_confidence_ratio"] = (results_df['confidence'] < self.confidence_threshold).mean()
        
        return results_df, metrics

# ===== config.py =====
# text_classifier/config.py
"""
Configuration utilities for the text classifier system.
"""

import json
from pathlib import Path
from typing import Dict, Any, Union, List, Optional


def load_config(config_path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load configuration from a JSON file.
    
    Args:
        config_path: Path to the configuration JSON file
        
    Returns:
        Configuration dictionary with defaults applied
    """
    config_path = Path(config_path)
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    with open(config_path) as f:
        user_config = json.load(f)
    
    # Apply defaults
    return get_config_with_defaults(user_config)


def validate_config(config: Dict[str, Any]) -> None:
    """
    Validate that required configuration parameters are present and valid.
    
    Args:
        config: Configuration dictionary
        
    Raises:
        ValueError: If required parameters are missing or invalid
    """
    required = ["file_path", "text_column", "id_column"]
    missing = [key for key in required if key not in config]
    
    if missing:
        raise ValueError(f"Missing required config parameters: {missing}")
    
    # Validate file exists
    file_path = Path(config["file_path"])
    if not file_path.exists():
        raise FileNotFoundError(f"Input file not found: {file_path}")
    
    # Validate backends
    valid_backends = {"ollama", "openai"}
    backend_keys = ["classifier_backend", "category_backend", "judge_backend"]
    
    for key in backend_keys:
        if key in config and config[key] is not None:
            if config[key] not in valid_backends:
                raise ValueError(
                    f"Invalid {key}: '{config[key]}'. Must be one of: {valid_backends}"
                )


def merge_configs(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """
    Merge two configuration dictionaries, with override taking precedence.
    
    Args:
        base: Base configuration
        override: Configuration values to override
        
    Returns:
        Merged configuration
    """
    result = base.copy()
    result.update(override)
    return result


def parse_categories(config: Dict[str, Any]) -> Optional[List[str]]:
    """
    Parse categories from config, handling both string and list formats.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        List of categories or None
    """
    raw = config.get("categories")
    if raw is None:
        return None
    
    if isinstance(raw, str):
        categories = [c.strip() for c in raw.split(",") if c.strip()]
    elif isinstance(raw, list):
        categories = [str(c).strip() for c in raw if str(c).strip()]
    else:
        raise ValueError("'categories' must be a list or comma-separated string")
    
    return categories if categories else None


# Default configuration values
DEFAULT_CONFIG = {
    "classifier_model": "gemma3n:latest",
    "category_model": None,
    "judge_model": "gemma3n:latest",
    "classifier_backend": "ollama",
    "category_backend": None,
    "judge_backend": None,  # ADD THIS LINE - it was missing!
    "multiclass": False,
    "n_samples": 100,
    "question_context": "",
    "validation_samples": None,
    # Error handling options
    "max_retries": 3,
    "retry_delay": 1,
    "error_category": "Classification Error",
    "fallback_categories": ["Positive", "Negative", "Neutral", "Other"]
}


def get_config_with_defaults(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Apply default values to configuration.
    
    Args:
        config: User-provided configuration
        
    Returns:
        Configuration with defaults applied
    """
    # Start with defaults
    result = DEFAULT_CONFIG.copy()
    
    # Override with user config
    result.update(config)
    
    # Apply cascading defaults for backend settings
    # If category_backend not specified, use classifier_backend
    if result.get("category_backend") is None:
        result["category_backend"] = result["classifier_backend"]
    
    # If judge_backend not specified, use classifier_backend
    if result.get("judge_backend") is None:
        result["judge_backend"] = result["classifier_backend"]
    
    # If category_model not specified, use classifier_model
    if result.get("category_model") is None:
        result["category_model"] = result["classifier_model"]
    
    return result

# ===== models.py =====
# text_classifier/models.py
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path


@dataclass
class ClassificationRun:
    """Represents a single classification run with metadata"""
    run_id: str
    timestamp: datetime
    config: Dict[str, Any]
    input_file: Path
    output_file: Optional[Path] = None
    categories: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'run_id': self.run_id,
            'timestamp': self.timestamp.isoformat(),
            'config': self.config,
            'input_file': str(self.input_file),
            'output_file': str(self.output_file) if self.output_file else None,
            'categories': self.categories,
            'metrics': self.metrics
        }


@dataclass
class ValidationRun:
    """Represents a validation run"""
    validation_id: str
    classification_run_id: str
    timestamp: datetime
    config: Dict[str, Any]
    results_file: Optional[Path] = None
    metrics: Dict[str, float] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'validation_id': self.validation_id,
            'classification_run_id': self.classification_run_id,
            'timestamp': self.timestamp.isoformat(),
            'config': self.config,
            'results_file': str(self.results_file) if self.results_file else None,
            'metrics': self.metrics
        }

# ===== storage.py =====
# text_classifier/storage.py
import json
from pathlib import Path
from typing import Optional, List, Dict, Any, TYPE_CHECKING
import pandas as pd
from datetime import datetime

if TYPE_CHECKING:
    from .models import ClassificationRun


class RunStorage:
    """Manages storage and retrieval of classification/validation runs"""
    
    def __init__(self, base_dir: Path = Path("./runs")):
        self.base_dir = base_dir
        self.base_dir.mkdir(exist_ok=True)
        self.metadata_file = self.base_dir / "metadata.json"
        self._load_metadata()
    
    def _load_metadata(self):
        if self.metadata_file.exists():
            self.metadata = json.loads(self.metadata_file.read_text())
        else:
            self.metadata = {"classification_runs": {}, "validation_runs": {}}
    
    def _save_metadata(self):
        self.metadata_file.write_text(json.dumps(self.metadata, indent=2))
    
    def save_classification_run(self, run: 'ClassificationRun', df: pd.DataFrame):
        """Save classification run data and metadata"""
        # Create run directory
        run_dir = self.base_dir / f"classification_{run.run_id}"
        run_dir.mkdir(exist_ok=True)
        
        # Save data
        output_file = run_dir / f"classified_{run.run_id}.csv"
        df.to_csv(output_file, index=False)
        run.output_file = output_file
        
        # Save config
        config_file = run_dir / "config.json"
        config_file.write_text(json.dumps(run.config, indent=2))
        
        # Update metadata
        self.metadata["classification_runs"][run.run_id] = run.to_dict()
        self._save_metadata()
        
        return output_file
    
    def get_classification_run(self, run_id: str) -> Optional['ClassificationRun']:
        """Retrieve classification run metadata"""
        if run_id in self.metadata["classification_runs"]:
            data = self.metadata["classification_runs"][run_id]
            # Import at runtime
            from .models import ClassificationRun
            return ClassificationRun(
                run_id=data['run_id'],
                timestamp=datetime.fromisoformat(data['timestamp']),
                config=data['config'],
                input_file=Path(data['input_file']),
                output_file=Path(data['output_file']) if data['output_file'] else None,
                categories=data['categories'],
                metrics=data['metrics']
            )
        return None
    
    def load_classification_data(self, run_id: str) -> Optional[pd.DataFrame]:
        """Load classification results DataFrame"""
        run = self.get_classification_run(run_id)
        if run and run.output_file and run.output_file.exists():
            return pd.read_csv(run.output_file)
        return None
    
    def list_runs(self, run_type: str = "classification") -> List[Dict[str, Any]]:
        """List all runs of a given type"""
        key = f"{run_type}_runs"
        return list(self.metadata.get(key, {}).values())    

# ===== validator.py =====
# text_classifier/validator.py
from typing import Dict, Any, Optional, List, Tuple
import pandas as pd
from tqdm import tqdm
import re

from .classifier import TextClassifier


class ClassificationValidator:
    """Handles validation of classifications"""
    
    def __init__(self, model_name: str, backend: str = "ollama"):
        self.classifier = TextClassifier(model_name, backend)

    def validate_single_multiclass(
        self, 
        text: str, 
        predictions: Dict[str, str], 
        categories: List[str]
    ) -> Tuple[Dict[str, int], str]:
        """Validate multiclass predictions for a single text"""
        
        # Build the predictions summary
        pred_summary = "\n".join([
            f"- {cat}: {predictions.get(cat, 'no')}" 
            for cat in categories
        ])
        
        system_prompt = """You are evaluating multiclass text classifications.
For each category prediction, rate its accuracy from 1-5.
Format your response as:
Category scores:
- [Category name]: [1-5] - [brief reason]
Overall: [1-5] - [overall assessment]"""
        
        user_prompt = f"""Original text: "{text}"

Predictions:
{pred_summary}

For each category, evaluate if the yes/no prediction is accurate.
Consider:
1. Does a "yes" prediction correctly identify that the text relates to this category?
2. Does a "no" prediction correctly identify that the text does NOT relate to this category?
3. Are there any missed categories (should be "yes" but marked "no")?
4. Are there any false positives (marked "yes" but shouldn't be)?

Rate each category prediction 1-5 where:
1 = completely wrong
3 = partially correct  
5 = perfectly accurate"""
        
        try:
            result = self.classifier.send_chat([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ])['message']['content']
            
            # Parse individual category scores
            scores = self._parse_multiclass_scores(result, categories)
            
            return scores, result
            
        except Exception as e:
            print(f"Error in validate_single_multiclass: {e}")
            # Return middle scores as default
            return {cat: 3 for cat in categories}, f"Validation error: {str(e)}"
    
    def _parse_multiclass_scores(self, text: str, categories: List[str]) -> Dict[str, int]:
        """Parse scores for each category from validation response"""
        import re
        
        scores = {}
        
        # Try to find scores for each category
        for cat in categories:
            # Look for patterns like "CategoryName: 4" or "CategoryName: 4/5"
            # Make pattern flexible to handle variations
            escaped_cat = re.escape(cat)
            patterns = [
                rf'{escaped_cat}\s*:\s*(\d)',  # Category: 4
                rf'{escaped_cat}.*?:\s*(\d)',   # Category blah blah: 4
                rf'{escaped_cat}.*?(\d)\s*/',   # Category 4/5
                rf'{escaped_cat}.*?(\d)\s*out', # Category 4 out of 5
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    score = int(match.group(1))
                    if 1 <= score <= 5:
                        scores[cat] = score
                        break
            
            # Default to 3 if not found
            if cat not in scores:
                scores[cat] = 3
                
        # Try to find overall score
        overall_patterns = [
            r'[Oo]verall\s*:\s*(\d)',
            r'[Oo]verall.*?(\d)\s*/',
            r'[Aa]verage\s*:\s*(\d)',
        ]
        
        for pattern in overall_patterns:
            match = re.search(pattern, text)
            if match:
                score = int(match.group(1))
                if 1 <= score <= 5:
                    scores['_overall'] = score
                    break
                    
        return scores

    def validate_single(
        self, 
        text: str, 
        predicted_category: str, 
        categories: List[str]
    ) -> Tuple[int, str]:
        """Validate a single classification with robust score parsing"""
        categories_str = ", ".join(sorted(categories))
        
        system_prompt = """You are evaluating text classifications. 
Rate classification quality from 1 to 5 and explain.
Format your response as:
Score: [number]
Explanation: [your explanation]"""
        
        user_prompt = f"""Original text: \"{text}\"
Predicted category: \"{predicted_category}\"
Available categories: {categories_str}

Rate overall classification quality from 1 (very poor) to 5 (excellent)."""
        
        try:
            result = self.classifier.send_chat([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ])['message']['content']
            
            # More robust score parsing
            score = self._parse_score(result)
            
            if score is None:
                print(f"Warning: Could not parse score from: {result[:100]}...")
                score = 3  # Default middle score
                
            return score, result
            
        except Exception as e:
            print(f"Error in validate_single: {e}")
            return 3, f"Validation error: {str(e)}"

    def _parse_score(self, text: str) -> Optional[int]:
        """Parse score from text with multiple strategies"""
        # Strategy 1: Look for "Score: X" pattern
        score_match = re.search(r'[Ss]core\s*:\s*(\d)', text)
        if score_match:
            score = int(score_match.group(1))
            if 1 <= score <= 5:
                return score
        
        # Strategy 2: Look for "X/5" or "X out of 5" pattern
        out_of_five = re.search(r'(\d)\s*(?:/|out of)\s*5', text)
        if out_of_five:
            score = int(out_of_five.group(1))
            if 1 <= score <= 5:
                return score
        
        # Strategy 3: Find first standalone digit 1-5
        for match in re.finditer(r'\b([1-5])\b', text):
            return int(match.group(1))
        
        # Strategy 4: Look for written numbers
        word_to_num = {'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}
        for word, num in word_to_num.items():
            if word in text.lower():
                return num
        
        return None

    def validate_classification_run(
        self,
        df: pd.DataFrame,
        text_column: str,
        category_column: Optional[str] = None,  # Make optional for multiclass
        categories: List[str] = None,
        sample_size: Optional[int] = None,
        multiclass: bool = None  # Auto-detect if not specified
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Validate a classification run (both single and multiclass)"""
        
        # Auto-detect multiclass if not specified
        if multiclass is None:
            # Check if we have category columns (multiclass) or a single category column
            multiclass = all(cat in df.columns for cat in categories) if categories else False
        
        # Sample if requested
        if sample_size and sample_size < len(df):
            df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
        
        # Prepare results dataframe
        results_df = df.copy()
        
        if multiclass:
            # Multiclass validation
            results_df['quality_scores'] = None
            results_df['explanation'] = None
            results_df['average_score'] = None
            
            for idx, row in tqdm(df.iterrows(), total=len(df), desc="Validating multiclass"):
                # Get predictions for this row
                predictions = {cat: row.get(cat, 'no') for cat in categories}
                
                scores, explanation = self.validate_single_multiclass(
                    row[text_column],
                    predictions,
                    categories
                )
                
                # Store results
                results_df.at[idx, 'quality_scores'] = scores
                results_df.at[idx, 'explanation'] = explanation
                
                # Calculate average score for this row
                cat_scores = [s for c, s in scores.items() if c != '_overall']
                avg_score = sum(cat_scores) / len(cat_scores) if cat_scores else 3
                results_df.at[idx, 'average_score'] = avg_score
                
        else:
            # Single category validation (existing code)
            results_df['quality_score'] = None
            results_df['explanation'] = None
            
            for idx, row in tqdm(df.iterrows(), total=len(df), desc="Validating"):
                score, explanation = self.validate_single(
                    row[text_column],
                    row[category_column],
                    categories
                )
                results_df.at[idx, 'quality_score'] = score
                results_df.at[idx, 'explanation'] = explanation
        
        # Calculate metrics
        if multiclass:
            metrics = {
                "total_validated": len(results_df),
                "multiclass": True,
                "average_score": results_df['average_score'].mean(),
                "score_distribution": results_df['average_score'].value_counts().to_dict()
            }
            
            # Add per-category average scores
            category_scores = {}
            for cat in categories:
                cat_scores = [
                    scores.get(cat, 3) 
                    for scores in results_df['quality_scores'] 
                    if scores and cat in scores
                ]
                if cat_scores:
                    category_scores[cat] = sum(cat_scores) / len(cat_scores)
            metrics["category_scores"] = category_scores
            
        else:
            metrics = {
                "total_validated": len(results_df),
                "multiclass": False,
                "average_score": results_df['quality_score'].astype(float).mean(),
                "score_distribution": results_df['quality_score'].value_counts().to_dict()
            }
        
        return results_df, metrics

