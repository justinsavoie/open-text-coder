# ===== api.py =====
# text_classifier/api.py
"""
High-level API for text classification system.
This module provides the main functions users should interact with.
"""

from pathlib import Path
from datetime import datetime
import uuid
import json
from typing import Dict, Any, Optional, List
import pandas as pd

from .models import ClassificationRun, ValidationRun
from .storage import RunStorage
from .classifier import TextClassifier
from .validator import ClassificationValidator
from .config import (
    load_config as _load_config,
    validate_config,
    get_config_with_defaults,
    parse_categories
)


def classify_texts(
    config: Dict[str, Any],
    run_id: Optional[str] = None,
    storage_dir: Path = Path("./runs")
) -> str:
    """
    Run text classification with proper storage.
    
    Args:
        config: Configuration dictionary with keys:
            - file_path: Path to input CSV
            - text_column: Column containing text
            - id_column: Column with unique IDs
            - categories: Optional list/string of categories
            - classifier_model: Model name (default: "gemma3n:latest")
            - backend: "ollama" or "openai" (default: "ollama")
            - multiclass: Enable multi-label (default: False)
            - n_samples: Samples for category generation (default: 100)
            - question_context: Context for category generation
            - category_model: Model for generating categories
        run_id: Optional run ID (auto-generated if None)
        storage_dir: Directory to store runs
        
    Returns:
        run_id: Unique identifier for this classification run
    """
    # Apply defaults and validate
    config = get_config_with_defaults(config)
    validate_config(config)
    
    # Generate run ID if not provided
    if run_id is None:
        run_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    
    # Initialize storage
    storage = RunStorage(storage_dir)
    
    # Load data
    df = pd.read_csv(config["file_path"],keep_default_na=False)
    
    # Parse categories from config
    categories = parse_categories(config)
    
    # Initialize classifier
    classifier = TextClassifier(
        config.get("classifier_model", "gemma3n:latest"),
        config.get("classifier_backend", config.get("backend", "ollama"))
    )
    
    # Run classification
    classified_df, final_categories, metrics = classifier.run_classification(
        df=df,
        text_column=config["text_column"],
        id_column=config["id_column"],
        categories=categories,
        multiclass=config.get("multiclass", False),
        n_samples=config.get("n_samples", 100),
        question_context=config.get("question_context", ""),
        category_model=config.get("category_model"),
        category_backend=config.get("category_backend", config.get("classifier_backend", config.get("backend", "ollama")))
    )
    
    # Create run record
    run = ClassificationRun(
        run_id=run_id,
        timestamp=datetime.now(),
        config=config,
        input_file=Path(config["file_path"]),
        categories=final_categories,
        metrics=metrics
    )
    
    # Save results
    output_file = storage.save_classification_run(run, classified_df)
    print(f"[*] Classification complete. Run ID: {run_id}")
    print(f"[*] Results saved to: {output_file}")
    
    return run_id


def validate_classification(
    classification_run_id: str,
    config: Optional[Dict[str, Any]] = None,
    sample_size: Optional[int] = None,
    storage_dir: Path = Path("./runs")
) -> str:
    """
    Validate a classification run using LLM-as-judge.
    
    Args:
        classification_run_id: ID of the classification run to validate
        config: Optional config to override validation settings
        sample_size: Number of samples to validate (None = all)
        storage_dir: Directory where runs are stored
        
    Returns:
        validation_id: Unique identifier for this validation run
    """
    # Initialize storage
    storage = RunStorage(storage_dir)
    
    # Load classification run
    class_run = storage.get_classification_run(classification_run_id)
    if not class_run:
        raise ValueError(f"Classification run {classification_run_id} not found")
    
    # Load classification data
    classified_df = storage.load_classification_data(classification_run_id)
    if classified_df is None:
        raise ValueError(f"Could not load data for run {classification_run_id}")
    
    # Use config from classification run if not provided
    if config is None:
        config = class_run.config.copy()
    
    # Override sample size if provided
    if sample_size is not None:
        config["validation_samples"] = sample_size
    
    # Initialize validator
    validator = ClassificationValidator(
        config.get("judge_model", "gemma3n:latest"),
        config.get("backend", "ollama")
    )
    
    # Determine category column
    if class_run.config.get("multiclass", False):
        # For multiclass, we'd need to validate each category separately
        # For now, let's skip multiclass validation
        raise NotImplementedError("Multiclass validation not yet implemented")
    else:
        category_column = "category"
    
    # Run validation
    validated_df, metrics = validator.validate_classification_run(
        df=classified_df,
        text_column=class_run.config["text_column"],
        category_column=category_column,
        categories=class_run.categories,
        sample_size=config.get("validation_samples")
    )
    
    # Generate validation ID
    val_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    
    # Save validation results
    val_dir = storage.base_dir / f"validation_{val_id}"
    val_dir.mkdir(exist_ok=True)
    
    output_file = val_dir / f"validation_{val_id}.csv"
    validated_df.to_csv(output_file, index=False)
    
    # Save validation metadata
    val_run = {
        "validation_id": val_id,
        "classification_run_id": classification_run_id,
        "timestamp": datetime.now().isoformat(),
        "config": config,
        "results_file": str(output_file),
        "metrics": metrics
    }
    
    storage.metadata["validation_runs"][val_id] = val_run
    storage._save_metadata()
    
    print(f"[*] Validation complete. ID: {val_id}")
    print(f"[*] Average quality score: {metrics['average_score']:.2f}")
    print(f"[*] Results saved to: {output_file}")
    
    return val_id


def load_classification_results(
    run_id: str,
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Load classification results for a given run.
    
    Args:
        run_id: Classification run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with classification results
    """
    storage = RunStorage(storage_dir)
    df = storage.load_classification_data(run_id)
    if df is None:
        raise ValueError(f"Could not load results for run {run_id}")
    return df


def load_validation_results(
    validation_id: str,
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Load validation results.
    
    Args:
        validation_id: Validation run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with validation results
    """
    storage = RunStorage(storage_dir)
    val_run = storage.metadata.get("validation_runs", {}).get(validation_id)
    if not val_run:
        raise ValueError(f"Validation run {validation_id} not found")
    
    results_file = Path(val_run["results_file"])
    if not results_file.exists():
        raise ValueError(f"Results file not found: {results_file}")
    
    return pd.read_csv(results_file)


def compare_runs(
    run_ids: List[str],
    storage_dir: Path = Path("./runs")
) -> pd.DataFrame:
    """
    Compare multiple classification runs.
    
    Args:
        run_ids: List of classification run IDs to compare
        storage_dir: Directory where runs are stored
        
    Returns:
        DataFrame with comparison metrics
    """
    storage = RunStorage(storage_dir)
    
    comparisons = []
    for run_id in run_ids:
        run = storage.get_classification_run(run_id)
        if run:
            comparison = {
                "run_id": run_id,
                "timestamp": run.timestamp,
                "model": run.config.get("classifier_model"),
                "backend": run.config.get("backend"),
                "multiclass": run.config.get("multiclass", False),
                "num_categories": len(run.categories),
                "total_rows": run.metrics.get("total_rows", 0),
                "classified_rows": run.metrics.get("classified_rows", 0)
            }
            
            # Add validation metrics if available
            validations = [v for v in storage.metadata.get("validation_runs", {}).values()
                          if v["classification_run_id"] == run_id]
            if validations:
                latest_val = max(validations, key=lambda x: x["timestamp"])
                comparison["validation_score"] = latest_val["metrics"]["average_score"]
                comparison["validated_samples"] = latest_val["metrics"]["total_validated"]
            
            comparisons.append(comparison)
    
    # Create comparison DataFrame
    comparison_df = pd.DataFrame(comparisons)
    
    print("\n=== Run Comparison ===")
    print(comparison_df.to_string())
    
    return comparison_df


def list_all_runs(
    run_type: str = "classification",
    storage_dir: Path = Path("./runs")
) -> List[Dict[str, Any]]:
    """
    List all runs of a given type.
    
    Args:
        run_type: "classification" or "validation"
        storage_dir: Directory where runs are stored
        
    Returns:
        List of run metadata dictionaries
    """
    storage = RunStorage(storage_dir)
    runs = storage.list_runs(run_type)
    
    # Sort by timestamp (newest first)
    runs.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
    
    return runs


def get_run_info(
    run_id: str,
    storage_dir: Path = Path("./runs")
) -> Dict[str, Any]:
    """
    Get detailed information about a specific run.
    
    Args:
        run_id: Classification or validation run ID
        storage_dir: Directory where runs are stored
        
    Returns:
        Dictionary with run information
    """
    storage = RunStorage(storage_dir)
    
    # Check classification runs
    run = storage.get_classification_run(run_id)
    if run:
        return {
            "type": "classification",
            "run": run.to_dict(),
            "validations": [v for v in storage.metadata.get("validation_runs", {}).values()
                           if v["classification_run_id"] == run_id]
        }
    
    # Check validation runs
    val_run = storage.metadata.get("validation_runs", {}).get(run_id)
    if val_run:
        return {
            "type": "validation",
            "run": val_run,
            "classification_run": storage.get_classification_run(
                val_run["classification_run_id"]
            ).to_dict() if val_run.get("classification_run_id") else None
        }
    
    raise ValueError(f"Run {run_id} not found")


# Convenience function for loading config from file
def load_config(config_path: str = "config.json") -> Dict[str, Any]:
    """Load configuration from JSON file."""
    return _load_config(config_path)

# ===== classifier.py =====
# text_classifier/classifier.py
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
from tqdm import tqdm
import os

try:
    import ollama
except ImportError:
    ollama = None
    
try:
    import openai
except ImportError:
    openai = None


class TextClassifier:
    def __init__(self, model_name: str, backend: str = "ollama"):
        self.model_name = model_name
        self.backend = backend

        if backend == "openai":
            if openai is None:
                raise ImportError("OpenAI package not installed. Run: pip install openai")
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("OPENAI_API_KEY environment variable not set")
            self.client = openai.OpenAI()  # <-- initialize once here
        elif backend == "ollama":
            if ollama is None:
                raise ImportError("Ollama package not installed. Run: pip install ollama")
    
    def send_chat(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Send chat request to LLM"""
        print("[CHATDEBUG] Messages:")
        for msg in messages:
            print(f"  - {msg['role']}: {msg['content']}")
    
        if self.backend == "openai":
            resp = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
            )
            content = resp.choices[0].message.content
            print("[CHATDEBUG] Response:\n", content)
            return {"message": {"content": content}}
        else:
            tempz = ollama.chat(model=self.model_name, messages=messages)
            print("[CHATDEBUG] Response:\n", tempz)
            return tempz
    
    def generate_categories(
        self,
        df: pd.DataFrame,
        text_column: str,
        n_samples: int = 100,
        question_context: str = ""
    ) -> List[str]:
        """Generate categories from sample responses"""
        print(f"[*] Sampling {n_samples} responses to generate categories...")
        sample = df[text_column].dropna().sample(n=min(n_samples, len(df)))
        sample_text = "\n\n".join(sample)
        
        prompt = (
            "The following are sample survey responses to the survey question:"
            f" {question_context}\n\n"
            f"{sample_text}\n\n"
            "Based on these responses, generate 5-15 mutually exclusive categories "
            "that summarise the main themes. Include a Don't know or Uncertain category "
            "if appropriate. Return a comma-separated list only."
        )
        reply = self.send_chat([{"role": "system", "content": prompt}])
        text = reply["message"]["content"].strip()
        if "\n" in text:
            text = text.split("\n")[-1]
        return [c.strip() for c in text.split(",") if c.strip()]
    
    def classify_single(self, text: str, categories: List[str], question_context: str = "") -> str:
        """Classify a single text"""
        prompt = (
            "You are a survey response classifier.\n"
            f"Survey question: {question_context}\n\n"
            f"Response:\n\"{text}\"\n\n"
            f"Choose the best category among:\n{', '.join(categories)}\n\n"
            "Return the category name only."
        )
        reply = self.send_chat([{"role": "system", "content": prompt}])
        return reply["message"]["content"].strip()
    
    def classify_single_multiclass(self, text: str, categories: List[str], question_context: str = "") -> Dict[str, str]:
        """Classify a single text into multiple categories"""
        category_list = "\n".join(f"{i+1}. {cat}" for i, cat in enumerate(categories))
        prompt = (
            f"You are a survey response classifier.\n"
            f"Survey question: {question_context}\n\n"
            f"Response:\n\"{text}\"\n\n"
            f"For each of the following categories, indicate if the response applies.\n\n"
            f"Categories:\n{category_list}\n\n"
            f"Reply with exactly {len(categories)} answers (\"yes\" or \"no\") separated by commas."
        )
        
        reply = self.send_chat([{"role": "system", "content": prompt}])
        answer = reply["message"]["content"].strip().lower()
        responses = [resp.strip() for resp in answer.split(",")]
        
        result = {}
        for i, cat in enumerate(categories):
            result[cat] = "yes" if i < len(responses) and "yes" in responses[i] else "no"
        return result
    
    def run_classification(
        self,
        df: pd.DataFrame,
        text_column: str,
        id_column: str,
        categories: Optional[List[str]] = None,
        multiclass: bool = False,
        n_samples: int = 100,
        question_context: str = "",
        category_model: Optional[str] = None,
        category_backend: Optional[str] = None  # NEW
    ) -> Tuple[pd.DataFrame, List[str], Dict[str, Any]]:
        """Run full classification pipeline"""
        # Generate categories if needed
        if categories is None:
            cat_classifier = TextClassifier(
                model_name=category_model or self.model_name,
                backend=category_backend or self.backend
            )
            categories = cat_classifier.generate_categories(
                df, text_column, n_samples, question_context
            )
            print(f"[*] Generated categories: {', '.join(categories)}")
        else:
            print(f"[*] Using provided categories: {', '.join(categories)}")
        
        # Classify responses
        rows = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Classifying"):
            txt = row[text_column]
            if pd.isna(txt):
                continue
            
            if multiclass:
                record = {
                    id_column: row[id_column],
                    text_column: txt,
                    **self.classify_single_multiclass(txt, categories, question_context)
                }
            else:
                record = {
                    id_column: row[id_column],
                    text_column: txt,
                    "category": self.classify_single(txt, categories, question_context)
                }
            rows.append(record)
        
        classified_df = pd.DataFrame(rows)
        
        # Calculate metrics
        metrics = {
            "total_rows": len(df),
            "classified_rows": len(classified_df),
            "multiclass": multiclass,
            "num_categories": len(categories)
        }
        
        if not multiclass:
            metrics["category_distribution"] = classified_df["category"].value_counts().to_dict()
        
        return classified_df, categories, metrics

# ===== config.py =====
# text_classifier/config.py
"""
Configuration utilities for the text classifier system.
"""

import json
from pathlib import Path
from typing import Dict, Any, Union, List, Optional


def load_config(config_path: Union[str, Path]) -> Dict[str, Any]:
    """
    Load configuration from a JSON file.
    
    Args:
        config_path: Path to the configuration JSON file
        
    Returns:
        Configuration dictionary
    """
    config_path = Path(config_path)
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    with open(config_path) as f:
        return json.load(f)


def validate_config(config: Dict[str, Any]) -> None:
    """
    Validate that required configuration parameters are present.
    
    Args:
        config: Configuration dictionary
        
    Raises:
        ValueError: If required parameters are missing
    """
    required = ["file_path", "text_column", "id_column"]
    missing = [key for key in required if key not in config]
    
    if missing:
        raise ValueError(f"Missing required config parameters: {missing}")
    
    # Validate file exists
    if not Path(config["file_path"]).exists():
        raise FileNotFoundError(f"Input file not found: {config['file_path']}")


def merge_configs(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """
    Merge two configuration dictionaries, with override taking precedence.
    
    Args:
        base: Base configuration
        override: Configuration values to override
        
    Returns:
        Merged configuration
    """
    result = base.copy()
    result.update(override)
    return result


def parse_categories(config: Dict[str, Any]) -> Optional[List[str]]:
    """
    Parse categories from config, handling both string and list formats.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        List of categories or None
    """
    raw = config.get("categories")
    if raw is None:
        return None
    
    if isinstance(raw, str):
        categories = [c.strip() for c in raw.split(",") if c.strip()]
    elif isinstance(raw, list):
        categories = [str(c).strip() for c in raw if str(c).strip()]
    else:
        raise ValueError("'categories' must be a list or comma-separated string")
    
    return categories if categories else None


# Default configuration values
DEFAULT_CONFIG = {
    "classifier_model": "gemma3n:latest",
    "category_model": None,  # Will use classifier_model if not specified
    "judge_model": "gemma3n:latest",
    "classifier_backend": "ollama",
    "category_backend": None,  # falls back to classifier_backend
    "multiclass": False,
    "n_samples": 100,
    "question_context": "",
    "validation_samples": None  # Validate all by default
}


def get_config_with_defaults(config: Dict[str, Any]) -> Dict[str, Any]:
    config = merge_configs(DEFAULT_CONFIG, config)
    
    # For backward compatibility
    if "classifier_backend" not in config:
        config["classifier_backend"] = config["backend"]
    if "category_backend" not in config:
        config["category_backend"] = config["classifier_backend"]
    
    return config

# ===== models.py =====
# text_classifier/models.py
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path


@dataclass
class ClassificationRun:
    """Represents a single classification run with metadata"""
    run_id: str
    timestamp: datetime
    config: Dict[str, Any]
    input_file: Path
    output_file: Optional[Path] = None
    categories: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'run_id': self.run_id,
            'timestamp': self.timestamp.isoformat(),
            'config': self.config,
            'input_file': str(self.input_file),
            'output_file': str(self.output_file) if self.output_file else None,
            'categories': self.categories,
            'metrics': self.metrics
        }


@dataclass
class ValidationRun:
    """Represents a validation run"""
    validation_id: str
    classification_run_id: str
    timestamp: datetime
    config: Dict[str, Any]
    results_file: Optional[Path] = None
    metrics: Dict[str, float] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'validation_id': self.validation_id,
            'classification_run_id': self.classification_run_id,
            'timestamp': self.timestamp.isoformat(),
            'config': self.config,
            'results_file': str(self.results_file) if self.results_file else None,
            'metrics': self.metrics
        }

# ===== storage.py =====
# text_classifier/storage.py
import json
from pathlib import Path
from typing import Optional, List, Dict, Any
import pandas as pd
from datetime import datetime


class RunStorage:
    """Manages storage and retrieval of classification/validation runs"""
    
    def __init__(self, base_dir: Path = Path("./runs")):
        self.base_dir = base_dir
        self.base_dir.mkdir(exist_ok=True)
        self.metadata_file = self.base_dir / "metadata.json"
        self._load_metadata()
    
    def _load_metadata(self):
        if self.metadata_file.exists():
            self.metadata = json.loads(self.metadata_file.read_text())
        else:
            self.metadata = {"classification_runs": {}, "validation_runs": {}}
    
    def _save_metadata(self):
        self.metadata_file.write_text(json.dumps(self.metadata, indent=2))
    
    def save_classification_run(self, run: 'ClassificationRun', df: pd.DataFrame):
        """Save classification run data and metadata"""
        # Create run directory
        run_dir = self.base_dir / f"classification_{run.run_id}"
        run_dir.mkdir(exist_ok=True)
        
        # Save data
        output_file = run_dir / f"classified_{run.run_id}.csv"
        df.to_csv(output_file, index=False)
        run.output_file = output_file
        
        # Save config
        config_file = run_dir / "config.json"
        config_file.write_text(json.dumps(run.config, indent=2))
        
        # Update metadata
        self.metadata["classification_runs"][run.run_id] = run.to_dict()
        self._save_metadata()
        
        return output_file
    
    def get_classification_run(self, run_id: str) -> Optional['ClassificationRun']:
        """Retrieve classification run metadata"""
        if run_id in self.metadata["classification_runs"]:
            data = self.metadata["classification_runs"][run_id]
            # Import here to avoid circular import
            from .models import ClassificationRun
            return ClassificationRun(
                run_id=data['run_id'],
                timestamp=datetime.fromisoformat(data['timestamp']),
                config=data['config'],
                input_file=Path(data['input_file']),
                output_file=Path(data['output_file']) if data['output_file'] else None,
                categories=data['categories'],
                metrics=data['metrics']
            )
        return None
    
    def load_classification_data(self, run_id: str) -> Optional[pd.DataFrame]:
        """Load classification results DataFrame"""
        run = self.get_classification_run(run_id)
        if run and run.output_file and run.output_file.exists():
            return pd.read_csv(run.output_file)
        return None
    
    def list_runs(self, run_type: str = "classification") -> List[Dict[str, Any]]:
        """List all runs of a given type"""
        key = f"{run_type}_runs"
        return list(self.metadata.get(key, {}).values())    

# ===== validator.py =====
# text_classifier/validator.py
from typing import Dict, Any, Optional, List, Tuple
import pandas as pd
from tqdm import tqdm

from .classifier import TextClassifier


class ClassificationValidator:
    """Handles validation of classifications"""
    
    def __init__(self, model_name: str, backend: str = "ollama"):
        self.classifier = TextClassifier(model_name, backend)
    
    def validate_single(
        self, 
        text: str, 
        predicted_category: str, 
        categories: List[str]
    ) -> Tuple[int, str]:
        """Validate a single classification"""
        categories_str = ", ".join(sorted(categories))
        
        system_prompt = "You are evaluating text classifications. Rate classification quality (1-5) and explain whether you agree."
        
        user_prompt = f"""For this text, I'll show you:
1. The original text
2. The predicted category
3. Available categories

Original text: \"{text}\"
Predicted category: \"{predicted_category}\"
Available categories: {categories_str}

Evaluate both the predicted category *and* the quality of the category scheme itself.
- If the categories are vague, overlapping, or poorly defined, give a low score.
- If the categories are meaningful and the prediction fits well, give a high score.

Rate overall classification quality from 1 (very poor) to 5 (excellent) and explain in one sentence.
"""
        
        result = self.classifier.send_chat([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ])['message']['content']
        
        # Parse score
        lines = result.strip().split('\n')
        score_line = next((l for l in lines if any(ch.isdigit() for ch in l)), None)
        score = int(next((ch for ch in score_line if ch.isdigit()), '0')) if score_line else None
        
        return score, result
    
    def validate_classification_run(
        self,
        df: pd.DataFrame,
        text_column: str,
        category_column: str,
        categories: List[str],
        sample_size: Optional[int] = None
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Validate a classification run"""
        # Sample if requested
        if sample_size and sample_size < len(df):
            df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
        
        # Validate each row
        results_df = df.copy()
        results_df['quality_score'] = None
        results_df['explanation'] = None
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Validating"):
            score, explanation = self.validate_single(
                row[text_column],
                row[category_column],
                categories
            )
            results_df.at[idx, 'quality_score'] = score
            results_df.at[idx, 'explanation'] = explanation
        
        # Calculate metrics
        metrics = {
            "total_validated": len(results_df),
            "average_score": results_df['quality_score'].astype(float).mean(),
            "score_distribution": results_df['quality_score'].value_counts().to_dict()
        }
        
        return results_df, metrics

