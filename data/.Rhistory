myf <- function(){
x1 <- sample(c(0,1),250,replace = TRUE, prob = c(0.7,0.3))
x2 <- sample(c(0,1),250,replace = TRUE, prob = c(0.5,0.5))
t.test(x1,x2)$statistic
}
mean(replicate(1000,myf()) < -1.96)
myf <- function() {
x1 <- rbinom(250, 1, prob = 0.3)  # Group 1: 30% success
x2 <- rbinom(250, 1, prob = 0.5)  # Group 2: 50% success
pval <- t.test(x1, x2)$p.value
return(pval < 0.05)  # Did we reject the null?
}
# Estimate power (proportion of rejections under alternative)
mean(replicate(1000, myf()))
# Required libraries
library(dplyr)
library(ggplot2)
# Define pairwise comparisons based on assumptions
comparisons <- tribble(
~label,                 ~p1,   ~p2,
"Passive vs Active",     0.40,  0.50,
"Active vs Treatment",   0.50,  0.60,
"Passive vs Treatment",  0.40,  0.60
)
comparisons <- comparisons %>%
rowwise() %>%
mutate(n_per_group = ceiling(power.prop.test(p1 = p1, p2 = p2, power = 0.8, sig.level = 0.05)$n))
comparisons
# Required libraries
library(dplyr)
library(ggplot2)
# Define pairwise comparisons with 0.10 differences
comparisons_90 <- tribble(
~label,                 ~p1,   ~p2,
"Passive vs Active",     0.40,  0.50,
"Active vs Treatment",   0.50,  0.60,
"Passive vs Treatment",  0.40,  0.60
)
# Compute required sample size per group for 90% power
comparisons_90 <- comparisons_90 %>%
rowwise() %>%
mutate(n_per_group = ceiling(power.prop.test(p1 = p1, p2 = p2, power = 0.9, sig.level = 0.05)$n))
comparisons_90
power.prop.test(p1 = p1, p2 = p2, power = 0.9, sig.level = 0.05)
power.prop.test(p1 = 0.4, p2 = 0.42, power = 0.9, sig.level = 0.05)
power.prop.test(p1 = 0.4, p2 = 0.5, power = 0.9, sig.level = 0.05)
ggplot(comparisons_90, aes(x = label, y = n_per_group, fill = label)) +
geom_col(show.legend = FALSE) +
geom_text(aes(label = n_per_group), vjust = -0.5) +
labs(title = "Sample Size per Group for 90% Power (10-point difference)",
x = "Comparison",
y = "Required N per group") +
ylim(0, max(comparisons_90$n_per_group) + 20) +
theme_minimal()
# Initial move Done
# Define paths
#source_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief_backup/"
#dest_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief/"
# List all .qmd files in the source directory
#qmd_files <- list.files(source_dir, pattern = "\\.qmd$", full.names = TRUE)
# Copy each file to the destination directory
#file.copy(from = qmd_files, to = dest_dir, overwrite = TRUE)
library(yaml)
library(stringr)
library(fs)
library(dplyr)
library(lubridate)
library(quarto)
# Set paths
entries_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief"
project_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief"
index_file <- file.path(project_dir, "index.qmd")
# List all .qmd files
qmd_files <- dir_ls(entries_dir, regexp = "\\.qmd$")
qmd_files <- grep("index.qmd",qmd_files,value = TRUE, invert = TRUE)
# deal with weird tocs
patterns_to_remove <- c("^\\s*toc:\\s*true\\s*$", "^\\s*toc-location:\\s*left\\s*$")
for (file in qmd_files) {
lines <- readLines(file, warn = FALSE)
cleaned_lines <- lines[!str_detect(lines, str_c(patterns_to_remove, collapse = "|"))]
writeLines(cleaned_lines, file)
}
# Function to extract YAML metadata
extract_yaml_info <- function(file) {
lines <- readLines(file, warn = FALSE)
yaml_start <- which(lines == "---")[1]
yaml_end <- which(lines == "---")[-1][1]
yaml_text <- paste(lines[(yaml_start+1):(yaml_end-1)], collapse = "\n")
yaml_parsed <- yaml.load(yaml_text)
title_raw <- yaml_parsed$title %||% "Untitled"
# Remove Markdown links from title if needed
title_clean <- str_match(title_raw, "\\[(.*?)\\]")[,2] %||% title_raw
date <- as.Date(yaml_parsed$date)
tibble(
title = title_clean,
file = file,
html = paste0(path_ext_remove(path_file(file)), ".html"),
date = date
)
}
entries_df <- bind_rows(lapply(qmd_files, extract_yaml_info)) %>%
mutate(
html = paste0("", path_ext_remove(path_file(file)), ".html")
) %>%
arrange(desc(date))
# Knit each .qmd to .html
for (file in entries_df$file) {
quarto_render(file)
}
# Create index content
intro_line <- "[The web of belief: the project](the-web-of-belief.html)\n\n"
essay_lines <- entries_df %>%
mutate(month_year = format(date, "%B %Y")) %>%
group_by(month_year) %>%
summarise(section = paste0("- [", title, "](", html, ") – ", month_year, collapse = "\n")) %>%
pull(section) %>%
paste(collapse = "\n")
index_content <- paste0(
"---\ntitle: \"The web of belief\"\nformat:\n  html:\n    embed-resources: true\n---\n\n",
intro_line, "\n",
essay_lines
)
# Write index.qmd
writeLines(index_content, index_file)
# Knit index.qmd
quarto_render(index_file)
# Initial move Done
# Define paths
#source_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief_backup/"
#dest_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief/"
# List all .qmd files in the source directory
#qmd_files <- list.files(source_dir, pattern = "\\.qmd$", full.names = TRUE)
# Copy each file to the destination directory
#file.copy(from = qmd_files, to = dest_dir, overwrite = TRUE)
library(yaml)
library(stringr)
library(fs)
library(dplyr)
library(lubridate)
library(quarto)
# Set paths
entries_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief"
project_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief"
index_file <- file.path(project_dir, "index.qmd")
# List all .qmd files
qmd_files <- dir_ls(entries_dir, regexp = "\\.qmd$")
qmd_files <- grep("index.qmd",qmd_files,value = TRUE, invert = TRUE)
# deal with weird tocs
patterns_to_remove <- c("^\\s*toc:\\s*true\\s*$", "^\\s*toc-location:\\s*left\\s*$")
for (file in qmd_files) {
lines <- readLines(file, warn = FALSE)
cleaned_lines <- lines[!str_detect(lines, str_c(patterns_to_remove, collapse = "|"))]
writeLines(cleaned_lines, file)
}
# Function to extract YAML metadata
extract_yaml_info <- function(file) {
lines <- readLines(file, warn = FALSE)
yaml_start <- which(lines == "---")[1]
yaml_end <- which(lines == "---")[-1][1]
yaml_text <- paste(lines[(yaml_start+1):(yaml_end-1)], collapse = "\n")
yaml_parsed <- yaml.load(yaml_text)
title_raw <- yaml_parsed$title %||% "Untitled"
# Remove Markdown links from title if needed
title_clean <- str_match(title_raw, "\\[(.*?)\\]")[,2] %||% title_raw
date <- as.Date(yaml_parsed$date)
tibble(
title = title_clean,
file = file,
html = paste0(path_ext_remove(path_file(file)), ".html"),
date = date
)
}
entries_df <- bind_rows(lapply(qmd_files, extract_yaml_info)) %>%
mutate(
html = paste0("", path_ext_remove(path_file(file)), ".html")
) %>%
arrange(desc(date))
# Knit each .qmd to .html
for (file in entries_df$file) {
quarto_render(file)
}
# Create index content
intro_line <- "[The web of belief: the project](the-web-of-belief.html)\n\n"
essay_lines <- entries_df %>%
mutate(month_year = format(date, "%B %Y")) %>%
group_by(month_year) %>%
summarise(section = paste0("- [", title, "](", html, ") – ", month_year, collapse = "\n")) %>%
pull(section) %>%
paste(collapse = "\n")
index_content <- paste0(
"---\ntitle: \"The web of belief\"\nformat:\n  html:\n    embed-resources: true\n---\n\n",
intro_line, "\n",
essay_lines
)
# Write index.qmd
writeLines(index_content, index_file)
# Knit index.qmd
quarto_render(index_file)
# Initial move Done
# Define paths
#source_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief_backup/"
#dest_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief/"
# List all .qmd files in the source directory
#qmd_files <- list.files(source_dir, pattern = "\\.qmd$", full.names = TRUE)
# Copy each file to the destination directory
#file.copy(from = qmd_files, to = dest_dir, overwrite = TRUE)
library(yaml)
library(stringr)
library(fs)
library(dplyr)
library(lubridate)
library(quarto)
# Set paths
entries_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief"
project_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief"
index_file <- file.path(project_dir, "index.qmd")
# List all .qmd files
qmd_files <- dir_ls(entries_dir, regexp = "\\.qmd$")
qmd_files <- grep("index.qmd",qmd_files,value = TRUE, invert = TRUE)
# deal with weird tocs
patterns_to_remove <- c("^\\s*toc:\\s*true\\s*$", "^\\s*toc-location:\\s*left\\s*$")
qmd_files_ <- grep("notes-conc",qmd_files,invert = TRUE, value = TRUE)
for (file in qmd_files_) {
lines <- readLines(file, warn = FALSE)
cleaned_lines <- lines[!str_detect(lines, str_c(patterns_to_remove, collapse = "|"))]
writeLines(cleaned_lines, file)
}
# Function to extract YAML metadata
extract_yaml_info <- function(file) {
lines <- readLines(file, warn = FALSE)
yaml_start <- which(lines == "---")[1]
yaml_end <- which(lines == "---")[-1][1]
yaml_text <- paste(lines[(yaml_start+1):(yaml_end-1)], collapse = "\n")
yaml_parsed <- yaml.load(yaml_text)
title_raw <- yaml_parsed$title %||% "Untitled"
# Remove Markdown links from title if needed
title_clean <- str_match(title_raw, "\\[(.*?)\\]")[,2] %||% title_raw
date <- as.Date(yaml_parsed$date)
tibble(
title = title_clean,
file = file,
html = paste0(path_ext_remove(path_file(file)), ".html"),
date = date
)
}
entries_df <- bind_rows(lapply(qmd_files, extract_yaml_info)) %>%
mutate(
html = paste0("", path_ext_remove(path_file(file)), ".html")
) %>%
arrange(desc(date))
# Knit each .qmd to .html
for (file in entries_df$file) {
quarto_render(file)
}
# Create index content
intro_line <- "[The web of belief: the project](the-web-of-belief.html)\n\n"
essay_lines <- entries_df %>%
mutate(month_year = format(date, "%B %Y")) %>%
group_by(month_year) %>%
summarise(section = paste0("- [", title, "](", html, ") – ", month_year, collapse = "\n")) %>%
mutate(month_year_ = lubridate::my(month_year)) %>% arrange(desc(month_year_)) %>%
pull(section) %>%
paste(collapse = "\n")
index_content <- paste0(
"---\ntitle: \"The web of belief\"\nformat:\n  html:\n    embed-resources: true\n---\n\n",
intro_line, "\n",
essay_lines
)
# Write index.qmd
writeLines(index_content, index_file)
# Knit index.qmd
quarto_render(index_file)
# Initial move Done
# Define paths
#source_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief_backup/"
#dest_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief/"
# List all .qmd files in the source directory
#qmd_files <- list.files(source_dir, pattern = "\\.qmd$", full.names = TRUE)
# Copy each file to the destination directory
#file.copy(from = qmd_files, to = dest_dir, overwrite = TRUE)
library(yaml)
library(stringr)
library(fs)
library(dplyr)
library(lubridate)
library(quarto)
# Set paths
entries_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief"
project_dir <- "/Users/justinsavoie/Dropbox (Personal)/Writing/thewebofbelief/thewebofbelief"
index_file <- file.path(project_dir, "index.qmd")
# List all .qmd files
qmd_files <- dir_ls(entries_dir, regexp = "\\.qmd$")
qmd_files <- grep("index.qmd",qmd_files,value = TRUE, invert = TRUE)
# deal with weird tocs
patterns_to_remove <- c("^\\s*toc:\\s*true\\s*$", "^\\s*toc-location:\\s*left\\s*$")
qmd_files_ <- grep("notes-conc",qmd_files,invert = TRUE, value = TRUE)
for (file in qmd_files_) {
lines <- readLines(file, warn = FALSE)
cleaned_lines <- lines[!str_detect(lines, str_c(patterns_to_remove, collapse = "|"))]
writeLines(cleaned_lines, file)
}
# Function to extract YAML metadata
extract_yaml_info <- function(file) {
lines <- readLines(file, warn = FALSE)
yaml_start <- which(lines == "---")[1]
yaml_end <- which(lines == "---")[-1][1]
yaml_text <- paste(lines[(yaml_start+1):(yaml_end-1)], collapse = "\n")
yaml_parsed <- yaml.load(yaml_text)
title_raw <- yaml_parsed$title %||% "Untitled"
# Remove Markdown links from title if needed
title_clean <- str_match(title_raw, "\\[(.*?)\\]")[,2] %||% title_raw
date <- as.Date(yaml_parsed$date)
tibble(
title = title_clean,
file = file,
html = paste0(path_ext_remove(path_file(file)), ".html"),
date = date
)
}
entries_df <- bind_rows(lapply(qmd_files, extract_yaml_info)) %>%
mutate(
html = paste0("", path_ext_remove(path_file(file)), ".html")
) %>%
arrange(desc(date))
# Knit each .qmd to .html
for (file in entries_df$file) {
quarto_render(file)
}
# Create index content
intro_line <- "[The web of belief: the project](the-web-of-belief.html)\n\n"
essay_lines <- entries_df %>%
mutate(month_year = format(date, "%B %Y")) %>%
group_by(month_year) %>%
summarise(section = paste0("- [", title, "](", html, ") – ", month_year, collapse = "\n")) %>%
mutate(month_year_ = lubridate::my(month_year)) %>% arrange(desc(month_year_)) %>%
pull(section) %>%
paste(collapse = "\n")
index_content <- paste0(
"---\ntitle: \"The web of belief\"\nformat:\n  html:\n    embed-resources: true\n---\n\n",
intro_line, "\n",
essay_lines
)
# Write index.qmd
writeLines(index_content, index_file)
# Knit index.qmd
quarto_render(index_file)
install.packages(c("ellmer", "usethis"))
getwd()
library(ellmer)
chat <- chat_openai(
model = "gpt-4.1-nano",
system_prompt =
"Generate tabular data based on the user's request.
Limit the data to 10 rows unless the user specifically requests more."
)
library(tidyverse)
read_csv("~/Downloads/gars1980-2023.csv")
read_csv("~/Downloads/gars1980-2023.csv") %>%
filter(Prenom_masculin %in% c("Maxence"))
read_csv("~/Downloads/gars1980-2023.csv") %>%
filter(Prenom_masculin %in% c("MAXENCE"))
read_csv("~/Downloads/gars1980-2023.csv") %>%
filter(Prenom_masculin %in% c("MAXENCE","AURELIEN")) %>% View()
read.csv("ACAN_ELECTION_FREENEXT_WHY_merged.csv")
temp <- read.csv("~/Desktop/ACAN_ELECTION_FREENEXT_WHY_merged.csv")
View(temp)
a <- Sys.time()
x1 <- 1
x2 <- 100
batch_size <- 5
results_list <- list()  # initialize list
for (i in seq(x1, x2, by = batch_size)) {
idx_end <- min(i + batch_size - 1, x2)  # make sure not to go past x2
to_classify <- temp$ACAN_ELECTION_FREENEXT_WHY_merged[i:idx_end]
ids <- temp$UniqueID[i:idx_end]
prompt3 <- paste0("Classify these answers:\n\n\n",
paste0(1:batch_size,". ",to_classify, collapse = "\n"),
"\n\n\nUse these choices:\n\n",
paste0(categories, collapse = "\n"),
"\n\nPut the classifications in a R vector. Just return a single R vector e.g., \n```R\nc(...)\n```\n\n. Wrap it response in triple backticks. No additional commentary.
")
cat(prompt3)
raw_classification <- generate("llama3.3:70b-instruct-q5_K_M",
prompt3,
stream = TRUE)
classification <- resp_process(raw_classification, "text")
matches <- regmatches(classification, regexpr("(?s)c\\(.*\\)", classification, perl = TRUE))
classification <- eval(parse(text = matches))
print(data.frame(ids = ids, classification = classification))
results_list[[length(results_list) + 1]] <- data.frame(ids = ids, classification = classification)
}
binded_df <- do.call(rbind, results_list)
write.csv(binded_df,"binded_df.csv")
library(ollamar)
temp <- read.csv("~/Desktop/ACAN_ELECTION_FREENEXT_WHY_merged.csv")
categories <- c("1. Lack of Trust in Government and Politicians", "2. Foreign Interference",
"3. Media Bias", "4. Electoral System Criticism (First-Past-the-Post/FPTP)",
"5. Corruption and Scandals", "6. Influence of Money and Special Interests",
"7. Voter Disenfranchisement", "8. Disillusionment with Democracy",
"9. Uncertainty or Lack of Knowledge", "10. Religious or Personal Beliefs",
"11. Partisan Bias", "12. Voting System and Process")
system_msg <- "You are a classification model. Given a list of 12 predefined categories and a single text response, return a valid JSON object with each key from \"1\" to \"14\" and each value either true or false, indicating whether the response belongs to that category. Do not explain. Return only the JSON object, like this: { \"1\": false, \"2\": true, ..., \"12\": false }"
make_prompt <- function(text, categories) {
cat_block <- paste(categories, collapse = "\n")
paste0(
"Here are 12 categories for supervised classification:\n\n",
cat_block, "\n\n",
"Classify the following response:\n\n",
"\"", text, "\"\n\n",
"It may fit multiple categories, or none.\n",
"Return only a JSON object with True or False for each category number (1 to 12)."
)
}
clean_text <- function(x) {
gsub("&#8217;", "'", x)
}
library(jsonlite)
results_list <- list()
it <- Sys.time()
i=1
print(i)
text_clean <- clean_text(temp$ACAN_ELECTION_FREENEXT_WHY_merged[i])
prompt <- make_prompt(text_clean, categories)
cat(prompt)
library(jsonlite)
results_list <- list()
it <- Sys.time()
for (i in 1:10) {
print(i)
text_clean <- clean_text(temp$ACAN_ELECTION_FREENEXT_WHY_merged[i])
prompt <- make_prompt(text_clean, categories)
raw <- generate(
model = "granite3.3:8b",
prompt = prompt,
system = system_msg,
stream = FALSE
)
response <- resp_process(raw, "text")
json_string <- gsub(".*(\\{.*\\}).*", "\\1", response)
results_list[[i]] <- tryCatch({
fromJSON(json_string)
}, error = function(e) {
warning(paste("Failed at row", i))
rep(NA, 14)
})
}
Sys.time()-it
1.18*50
prompt
cat(prompt)
library(ollamar)
test <- generate("llama3.1", "say better Looking at the data throuygh an information consumption lens, we see that it's obviosuly increaslingly challenging to determine or talk of the what the 'average Canadian' view is; and we can note that it's difficultly challenging to govern when there are at least three different information oriented based ways to cosume information.", stream = TRUE)
library(ollamar)
test <- generate("cogito:14b", "say better Looking at the data throuygh an information consumption lens, we see that it's obviosuly increaslingly challenging to determine or talk of the what the 'average Canadian' view is; and we can note that it's difficultly challenging to govern when there are at least three different information oriented based ways to cosume information.", stream = TRUE)
setwd("~/Documents/personal_repos/ai_creative/open-text-coder/")
setwd("~/Documents/personal_repos/ai_creative/open-text-coder/data/")
library(tidyverse)
library(haven)
df <- read_stata("~/Downloads/2021 Canadian Election Study v2.0.dta")
set.seed(232)
df <- read_stata("~/Downloads/2021 Canadian Election Study v2.0.dta")
set.seed(232)
df %>%
select(cps21_ResponseId,cps21_imp_iss) %>%
sample_n(40) %>%
write_csv("data-cps21-40.csv")
df <- read_stata("~/Downloads/2021 Canadian Election Study v2.0.dta")
set.seed(232)
df %>%
select(cps21_ResponseId,cps21_imp_iss) %>%
sample_n(400)
read.csv("data-cps21-GOLD-raw.csv")
read.csv("data-cps21-GOLD-raw.csv") %>% pull(cps21_ResponseId)
ids_in_gold <- read.csv("data-cps21-GOLD-raw.csv") %>% pull(cps21_ResponseId)
ids_in_gold
df %>%
filter(cps21_ResponseId %in% ids_in_gold)
setwd("~/Documents/personal_repos/ai_creative/open-text-coder/data/")
library(tidyverse)
library(haven)
df <- read_stata("~/Downloads/2021 Canadian Election Study v2.0.dta")
set.seed(232)
df %>%
select(cps21_ResponseId,cps21_imp_iss) %>%
sample_n(400) %>%
write_csv("data-cps21-GOLD-raw.csv")
df <- read_stata("~/Downloads/2021 Canadian Election Study v2.0.dta")
set.seed(232)
df %>%
select(cps21_ResponseId,cps21_imp_iss) %>%
sample_n(400) %>%
write_csv("data-cps21-GOLD-raw.csv")
ids_in_gold <- read.csv("data-cps21-GOLD-raw.csv") %>% pull(cps21_ResponseId)
ids_in_gold
set.seed(111)
ids_in_gold <- read.csv("data-cps21-GOLD-raw.csv") %>% pull(cps21_ResponseId)
df %>%
filter(!(cps21_ResponseId %in% ids_in_gold)) %>%
select(cps21_ResponseId,cps21_imp_iss) %>%
sample_n(400) %>%
write_csv("data-cps21.csv")
setwd("~/Documents/personal_repos/ai_creative/open-text-coder/data/")
library(tidyverse)
library(haven)
df <- read_stata("~/Downloads/2021 Canadian Election Study v2.0.dta")
df %>%
select(cps21_ResponseId,cps21_imp_iss) %>%
filter(complete.cases(.))
df %>%
select(cps21_ResponseId,cps21_imp_iss) %>%
filter(complete.cases(.)) %>%
write_csv("data-cps21-FULL.csv")
